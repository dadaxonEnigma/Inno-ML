{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQWLygmO75Mq"
   },
   "source": [
    "# Lab 5 : More on Recurrent neural networks (LSTM)\n",
    "```\n",
    "- [S25] Advanced Machine Learning, Innopolis University\n",
    "- Teaching Assistant: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "\n",
    "```\n",
    "Lab Plan\n",
    "1. LSTM basics\n",
    "2. Application of LSTM\n",
    "3. Self practice tasks\n",
    "```\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcdKLBsZ8mOD"
   },
   "source": [
    "## 0. Recap\n",
    "\n",
    "![](http://karpathy.github.io/assets/rnn/diags.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CEDP2Z38xEA"
   },
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HjJf-itv7vrX",
    "outputId": "cc9b3bdf-3980-43c3-ea75-078779562e4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "simple_sequence = torch.Tensor([[0.3,1.9,4.5],[0.4,0.1,0.23],[0.7,0.91,0.43], [0.34,0.01,0.002]])\n",
    "simple_sequence = simple_sequence.unsqueeze(0)\n",
    "simple_sequence.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5m4x8BY86Wh"
   },
   "source": [
    "## 1. LSTM basics\n",
    "\n",
    "The `simple_sequence` variable represents a sequence of length 4, where each element (time-stamp) is represented by a feature vector of length 3. LSTM calculations are defined as:\n",
    "\n",
    "![](https://media.licdn.com/dms/image/v2/C5612AQH5Im8XrvLmYQ/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1564974698831?e=2147483647&v=beta&t=4sP9wrqZVaKsUt8NLXwuN4hfYc0m8RKI3a5g_jUW2xc)\n",
    "\n",
    "\n",
    "$$i_{t} = \\sigma\\left(W_{ii}x_t + b_{ii} + W_{hi}h_{t-1} + b_{hi} \\right)$$\n",
    "$$f_t = \\sigma \\left( W_{if}x_t + b_{if} + W_{hf}h_{t-1} + b_{hf} \\right)$$\n",
    "$$g_t = tanh(W_{ig}x_t + b_{ig} + W_{hg}h_{t-1} + b_{hg})$$\n",
    "$$o_t = \\sigma \\left( W_{io}x_t + b_{io} + W_{ho}h_{t-1} + h_{ho}\\right)$$\n",
    "$$c_t = f_t \\odot c_{t-1} + i_t \\odot g_t$$\n",
    "$$h_t = o_t \\odot tanh(c_t)$$\n",
    "\n",
    "where $h_t$ represents the hidden state at time $t$; $c_t$ cell cell state at time $t$, $x_t$ is the input at time $t$, $h_{t-1}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time 0, and $i_t$, $f_t$, $g_t$, $o_t$ are the input, forget, cell, and output gates, respectively.\n",
    "\n",
    " <br>\n",
    "Lets see whats inside Pytorch and compare with our theory\n",
    "\n",
    "**Note:** For simplicity, the bias is set to zeros and weights set to ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ay97aV_f9FMb",
    "outputId": "7264ecc6-21f6-4607-a58b-a73a7c1cf6db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight_ih_l0',\n",
       "              tensor([[-0.9475, -0.6130, -0.1291],\n",
       "                      [-0.4107,  1.3931, -0.0984],\n",
       "                      [ 1.6791, -0.9381, -0.4899],\n",
       "                      [ 0.2811, -0.2813,  0.4779]])),\n",
       "             ('weight_hh_l0',\n",
       "              tensor([[ 0.8846],\n",
       "                      [-0.4928],\n",
       "                      [ 0.4776],\n",
       "                      [ 0.0807]])),\n",
       "             ('bias_ih_l0', tensor([0., 0., 0., 0.])),\n",
       "             ('bias_hh_l0', tensor([0., 0., 0., 0.]))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(20)\n",
    "hidden_size = 1\n",
    "simple_lstm_layer = torch.nn.LSTM(input_size=3, hidden_size=hidden_size, bidirectional=False, num_layers=1, batch_first=True)\n",
    "\n",
    "\n",
    "share_weight = torch.randn(simple_lstm_layer.weight_ih_l0.shape, dtype = torch.float)\n",
    "simple_lstm_layer.weight_ih_l0 = torch.nn.Parameter(share_weight)\n",
    "\n",
    "# bias set to zeros\n",
    "simple_lstm_layer.bias_ih_l0 = torch.nn.Parameter(torch.zeros(simple_lstm_layer.bias_ih_l0.shape))\n",
    "simple_lstm_layer.bias_hh_l0 = torch.nn.Parameter(torch.zeros(simple_lstm_layer.bias_ih_l0.shape))\n",
    "\n",
    "lstm_pytorch_output = simple_lstm_layer(simple_sequence[0][0].unsqueeze(dim=0).unsqueeze(dim=0))\n",
    "simple_lstm_layer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAGk_INpElZD"
   },
   "source": [
    "### Whole sequence output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZJiylw80wSH"
   },
   "outputs": [],
   "source": [
    "output, (hidden, cell) = simple_lstm_layer(simple_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gicPct9wED-c"
   },
   "source": [
    "### 1.2 Extract / define the calculation variables (weights \\& bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEaUqCUY9N_x"
   },
   "outputs": [],
   "source": [
    "W_ii, W_if, W_ig, W_io = simple_lstm_layer.weight_ih_l0.split(hidden_size, dim=0)\n",
    "b_ii, b_if, b_ig, b_io = simple_lstm_layer.bias_ih_l0.split(hidden_size, dim=0)\n",
    "\n",
    "W_hi, W_hf, W_hg, W_ho = simple_lstm_layer.weight_hh_l0.split(hidden_size, dim=0)\n",
    "b_hi, b_hf, b_hg, b_ho = simple_lstm_layer.bias_hh_l0.split(hidden_size, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9kRbMM3EZAe"
   },
   "source": [
    "### 2.2 Calculations\n",
    "\n",
    "$i_{t} = \\sigma\\left(W_{ii}x_t + b_{ii} + W_{hi}h_{t-1} + b_{hi} \\right)$ <br>\n",
    "$f_t = \\sigma \\left( W_{if}x_t + b_{if} + W_{hf}h_{t-1} + b_{hf} \\right)$ <br>\n",
    "$g_t = tanh(W_{ig}x_t + b_{ig} + W_{hg}h_{t-1} + b_{hg})$ <br>\n",
    "$o_t = \\sigma \\left( W_{io}x_t + b_{io} + W_{ho}h_{t-1} + h_{ho}\\right)$ <br>\n",
    "$c_t = f_t \\odot c_{t-1} + i_t \\odot g_t$ <br>\n",
    "$h_t = o_t \\odot tanh(c_t)$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdB62UN3DdFI"
   },
   "outputs": [],
   "source": [
    "input_x = simple_sequence[0][0].unsqueeze(0)\n",
    "prev_h = torch.zeros((1, hidden_size))\n",
    "prev_c = torch.zeros((1, hidden_size))\n",
    "\n",
    "i_t = torch.sigmoid(F.linear(input_x,W_ii,b_ii )+ F.linear(prev_h, W_hi,b_hi))\n",
    "f_t = torch.sigmoid(F.linear(input_x,W_if,b_if )+ F.linear(prev_h, W_hf,b_hf))\n",
    "g_t = torch.tanh(F.linear(input_x,W_ig,b_ig )+ F.linear(prev_h, W_hg,b_hg))\n",
    "o_t = torch.sigmoid(F.linear(input_x,W_io,b_io )+ F.linear(prev_h, W_ho,b_ho))\n",
    "c_t = f_t * prev_c + i_t * g_t\n",
    "h_t = o_t * torch.tanh(c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRkOgN_FEyl4"
   },
   "source": [
    "### 2.3 Comapre manual calculations with Pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4PdDoF3sMVJ3",
    "outputId": "aa429ac2-b187-41b1-ce27-dd6418ab960e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0976], grad_fn=<SelectBackward0>),\n",
       " tensor([[-0.0976]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.squeeze(0)[0], h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLhmYKNVqboO"
   },
   "source": [
    "**Task:** Calculate the outputs for the rest of the full sentence -> `simple_sequence` manually and compare with PyTorch output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMotALke62a1",
    "outputId": "3cb9f4e9-0c71-4c90-9dea-deaf3b3f20ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4000, 0.1000, 0.2300])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_sequence.squeeze(0).squeeze(0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kuivnScoOiLm",
    "outputId": "382a25e7-dde4-414f-f3e8-cc8a5a05b439"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0976]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.0470]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.0490]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.1372]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prev_h = torch.zeros((1, hidden_size))\n",
    "prev_c = torch.zeros((1, hidden_size))\n",
    "\n",
    "for i in range(simple_sequence.shape[1]):\n",
    "  input_x = simple_sequence[0][i].unsqueeze(0)\n",
    "  h_t, c_t = simple_lstm_layer(i_t,f_t,g_t,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3HrBshLEMEk",
    "outputId": "7c156e89-1495-4569-9684-3d39c22e0bcd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0976],\n",
       "         [ 0.0470],\n",
       "         [ 0.0490],\n",
       "         [ 0.1372]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAwUl3tNOtgC"
   },
   "source": [
    "## 2. Application of LSTM (Sentiment Analysis)\n",
    "\n",
    "### 2.1 Dataset Description\n",
    "\n",
    "[IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/) having 50K movie reviews for natural language processing or Text analytics. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LFMR6Zv8Ovw5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in e:\\anaconda\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in e:\\anaconda\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\anaconda\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in e:\\anaconda\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in e:\\anaconda\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in e:\\anaconda\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in e:\\anaconda\\lib\\site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in e:\\anaconda\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in e:\\anaconda\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in e:\\anaconda\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in e:\\anaconda\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in e:\\anaconda\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in e:\\anaconda\\lib\\site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in e:\\anaconda\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\anaconda\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in e:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in e:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\anaconda\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\anaconda\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "\n",
    "import collections\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pePNLd1fPMf5"
   },
   "source": [
    "### 2.2 Get Dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EZhcWJANPMG7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7f999b894c4ebfb87ae832d3a6a3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587bfeb8b4ce47c8808a1c7fee690db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea1d573191245fd9e9f12d1875405c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a786c91f77c64adfaec66c9fab1c22bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efefb177d1aa4eb0a7f0b2a26392825d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a77a512c3624ada88731f2584a610a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991fe6c368d44865bc7175ef83050ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data, test_data = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utkyerxYPXaf"
   },
   "source": [
    "### 2.3 Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ok9kuURyPbKB"
   },
   "outputs": [],
   "source": [
    "max_text_length = 128\n",
    "\n",
    "train_data, test_data = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Luvc9zxcPjb6"
   },
   "source": [
    "### 2.4 Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIBc3LQ7PgT1"
   },
   "outputs": [],
   "source": [
    "min_freq = 5\n",
    "special_tokens = [\"<unk>\", \"<pad>\"]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return {\"tokens\": [text.split() for text in examples[\"text\"]]}def tokenize_function(examples):\n",
    "    return {\"tokens\": [text.split() for text in examples[\"text\"]]}\n",
    "\n",
    "train_data = train_data.map(tokenize_function, batched=True)\n",
    "test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Build vocabulary\n",
    "token_counts = Counter()\n",
    "for example in train_data[\"tokens\"]:\n",
    "    token_counts.update(example)\n",
    "\n",
    "# Filter tokens by min_freq and add special tokens\n",
    "filtered_tokens = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "vocab_list = special_tokens + filtered_tokens\n",
    "    \n",
    "vocab = {token: idx for idx, token in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myeU2qWbPmjv"
   },
   "source": [
    "### 2.5 Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpCxcOFgPhqr"
   },
   "outputs": [],
   "source": [
    "train_data = None\n",
    "test_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1O7io-CjPs9m"
   },
   "source": [
    "### 2.6 Creating Dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mi3ulScaPwOc"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_data_loader = None\n",
    "test_data_loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skldsoALP3xB"
   },
   "source": [
    "### 2.7 Define LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "midQ02LJP4KW"
   },
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_index, n_layers=1, bidirectional=False):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n",
    "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "  def forward(self, ids):\n",
    "      embedded = self.embedding(ids)\n",
    "      output, (hidden, cell) = self.lstm(embedded)\n",
    "      prediction = self.fc(hidden[-1])\n",
    "      return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4OwfcvEQK1e"
   },
   "source": [
    "### 2.8 Model training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6yhwVJ5QLNu"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300\n",
    "hidden_dim = 32\n",
    "output_dim = None # TODO\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 5e-4\n",
    "\n",
    "model = SentimentLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    pad_index=pad_index,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdmztP7MQYIN"
   },
   "source": [
    "### 2.9 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CAOfZ5bQgwm"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(prediction, label):\n",
    "  batch_size, _ = prediction.shape\n",
    "  predicted_classes = prediction.argmax(dim=-1)\n",
    "  correct_predictions = predicted_classes.eq(label).sum()\n",
    "  accuracy = correct_predictions / batch_size\n",
    "  return accuracy\n",
    "\n",
    "def evaluate(dataloader, model, criterion, device):\n",
    "  model.eval()\n",
    "  epoch_losses = []\n",
    "  epoch_accs = []\n",
    "  # TODO: Write your code here\n",
    "  return np.mean(epoch_losses), np.mean(epoch_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSNdTWWgQjjD"
   },
   "source": [
    "### 2.10 Model training Loop\n",
    "\n",
    "**Task** : Add model evaluation (use `test_data_loader`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2dlz8ftoQnx-",
    "outputId": "5b610782-af31-4817-c8c6-c03c6c90c5c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 391/391 [00:04<00:00, 79.26it/s]\n",
      "evaluating...: 100%|██████████| 391/391 [00:05<00:00, 74.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train Loss: 0.694, Train Acc: 0.520, Test Loss: 0.692, Test Acc: 0.529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 391/391 [00:07<00:00, 51.33it/s]\n",
      "evaluating...: 100%|██████████| 391/391 [00:04<00:00, 92.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.673, Train Acc: 0.587, Test Loss: 0.685, Test Acc: 0.550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 391/391 [00:03<00:00, 125.20it/s]\n",
      "evaluating...: 100%|██████████| 391/391 [00:01<00:00, 249.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 0.639, Train Acc: 0.647, Test Loss: 0.678, Test Acc: 0.576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 391/391 [00:02<00:00, 133.95it/s]\n",
      "evaluating...: 100%|██████████| 391/391 [00:01<00:00, 260.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 0.569, Train Acc: 0.720, Test Loss: 0.669, Test Acc: 0.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 391/391 [00:03<00:00, 113.53it/s]\n",
      "evaluating...: 100%|██████████| 391/391 [00:01<00:00, 256.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 0.494, Train Acc: 0.775, Test Loss: 0.671, Test Acc: 0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 391/391 [00:02<00:00, 131.17it/s]\n",
      "evaluating...: 100%|██████████| 391/391 [00:01<00:00, 257.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 0.406, Train Acc: 0.829, Test Loss: 0.735, Test Acc: 0.627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 391/391 [00:02<00:00, 131.02it/s]\n",
      "evaluating...: 100%|██████████| 391/391 [00:01<00:00, 199.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 0.375, Train Acc: 0.847, Test Loss: 0.693, Test Acc: 0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 391/391 [00:03<00:00, 120.02it/s]\n",
      "evaluating...: 100%|██████████| 391/391 [00:01<00:00, 258.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss: 0.287, Train Acc: 0.892, Test Loss: 0.713, Test Acc: 0.686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 391/391 [00:02<00:00, 133.57it/s]\n",
      "evaluating...: 100%|██████████| 391/391 [00:01<00:00, 252.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss: 0.228, Train Acc: 0.919, Test Loss: 0.771, Test Acc: 0.686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 391/391 [00:03<00:00, 110.81it/s]\n",
      "evaluating...: 100%|██████████| 391/391 [00:01<00:00, 250.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss: 0.202, Train Acc: 0.932, Test Loss: 0.831, Test Acc: 0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for ep in range(n_epochs):\n",
    "  model.train()\n",
    "  epoch_losses = []\n",
    "  epoch_accs = []\n",
    "  for batch in tqdm.tqdm(train_data_loader, desc=\"training...\"):\n",
    "    optimizer.zero_grad()\n",
    "    ids = None\n",
    "    label = None\n",
    "\n",
    "    prediction = model(ids)\n",
    "    loss = criterion(prediction, label)\n",
    "    accuracy = get_accuracy(prediction, label)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_losses.append(loss.item())\n",
    "    epoch_accs.append(accuracy.item())\n",
    "  test_loss, test_acc = evaluate(test_data_loader, model, criterion=criterion, device=device)\n",
    "  print(f'[Epoch {ep}] Train Loss: {np.mean(epoch_losses):.3f}, Train Acc: {np.mean(epoch_accs):.3f}, Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32KOGoWTRSMV"
   },
   "source": [
    "## 3. Tasks\n",
    "\n",
    "```\n",
    "Task 1\n",
    "Implement and train a LSTM neural network for sentiment analysis using IMDb dataset and the following architecture:\n",
    "- LSTM should be bidirectional\n",
    "- LSTM should be Multi-layered\n",
    "- LSTM should be use Regularization (i.e Dropout)\n",
    "```\n",
    "\n",
    "<hr>\n",
    "\n",
    "```\n",
    "Task 2\n",
    "Implement, train and test a LSTM model for Part-of-speech tagging task.\n",
    "```\n",
    "\n",
    "**Task 2 Datasets**: [Train](https://www.dropbox.com/s/x9n6f9o9jl7pno8/train_pos.txt?dl=1), [Test](https://www.dropbox.com/s/v8nccvq7jewcl8s/test_pos.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "UXaYxAD7HkYC"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM,TimeDistributed,  Dense, Dropout, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000  # Dictionary size\n",
    "max_length = 200  # Maximum review length\n",
    "embedding_dim = 128  # Size of the word vector representation\n",
    "lstm_units = 64  # Number of neurons in the LSTM layer\n",
    "dropout_rate = 0.5  # Regularization\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load IMDb dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences to ensure uniform input size\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding multiple LSTM layers with Bidirectional wrapper\n",
    "for _ in range(num_layers - 1):\n",
    "    model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
    "    model.add(Dropout(dropout_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final LSTM layer\n",
    "model.add(Bidirectional(LSTM(lstm_units)))\n",
    "model.add(Dropout(dropout_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "391/391 [==============================] - 244s 599ms/step - loss: 0.4241 - accuracy: 0.7992 - val_loss: 0.3466 - val_accuracy: 0.8519\n",
      "Epoch 2/5\n",
      "391/391 [==============================] - 233s 595ms/step - loss: 0.2262 - accuracy: 0.9158 - val_loss: 0.4391 - val_accuracy: 0.8464\n",
      "Epoch 3/5\n",
      "391/391 [==============================] - 238s 609ms/step - loss: 0.1593 - accuracy: 0.9455 - val_loss: 0.4996 - val_accuracy: 0.8129\n",
      "Epoch 4/5\n",
      "391/391 [==============================] - 232s 593ms/step - loss: 0.1088 - accuracy: 0.9647 - val_loss: 0.5187 - val_accuracy: 0.8248\n",
      "Epoch 5/5\n",
      "391/391 [==============================] - 219s 560ms/step - loss: 0.0727 - accuracy: 0.9770 - val_loss: 0.6009 - val_accuracy: 0.8302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22005c0fc40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 66s 84ms/step - loss: 0.6009 - accuracy: 0.8302\n",
      "Test Accuracy: 0.8302\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    sentences, sentence, tags, tag_seq = [], [], [], []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                sentences.append(sentence)\n",
    "                tags.append(tag_seq)\n",
    "                sentence, tag_seq = [], []\n",
    "            else:\n",
    "                word, tag = line.split()\n",
    "                sentence.append(word)\n",
    "                tag_seq.append(tag)\n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_tags = load_data(\"lab5/train_pos.txt\")\n",
    "test_sentences, test_tags = load_data(\"lab5/test_pos.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. Creating Token dictionaries ===\n",
    "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}  # Special Characters\n",
    "tag2idx = {\"<PAD>\": 0}\n",
    "for sent in train_sentences:\n",
    "    for word in sent:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "for tag_seq in train_tags:\n",
    "    for tag in tag_seq:\n",
    "        if tag not in tag2idx:\n",
    "            tag2idx[tag] = len(tag2idx)\n",
    "\n",
    "idx2tag = {i: tag for tag, i in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Converting to numeric tensors ===\n",
    "max_len = max(len(s) for s in train_sentences)  # Maximum sentence length\n",
    "\n",
    "X_train = [[word2idx.get(word, 1) for word in sent] for sent in train_sentences]\n",
    "X_test = [[word2idx.get(word, 1) for word in sent] for sent in test_sentences]\n",
    "\n",
    "y_train = [[tag2idx[tag] for tag in tags] for tags in train_tags]\n",
    "y_test = [[tag2idx[tag] for tag in tags] for tags in test_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. Padding Sequences ===\n",
    "X_train = pad_sequences(X_train, maxlen=max_len, padding=\"post\")\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "y_train = pad_sequences(y_train, maxlen=max_len, padding=\"post\")\n",
    "y_test = pad_sequences(y_test, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "# One-hot encoding для меток\n",
    "y_train = [to_categorical(i, num_classes=len(tag2idx)) for i in y_train]\n",
    "y_test = [to_categorical(i, num_classes=len(tag2idx)) for i in y_test]\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. Creating a BiLSTM model ===\n",
    "vocab_size = len(word2idx)\n",
    "tag_size = len(tag2idx)\n",
    "embedding_dim = 128\n",
    "lstm_units = 64\n",
    "dropout_rate = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):  # Running on the GPU\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "        Bidirectional(LSTM(units=lstm_units, return_sequences=True)),\n",
    "        Dropout(dropout_rate),\n",
    "        TimeDistributed(Dense(tag_size, activation=\"softmax\"))  # Output layer of POS tags\n",
    "    ])\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "280/280 [==============================] - 35s 109ms/step - loss: 0.9957 - accuracy: 0.7617 - val_loss: 0.5323 - val_accuracy: 0.8606\n",
      "Epoch 2/5\n",
      "280/280 [==============================] - 29s 103ms/step - loss: 0.2502 - accuracy: 0.9419 - val_loss: 0.1056 - val_accuracy: 0.9750\n",
      "Epoch 3/5\n",
      "280/280 [==============================] - 28s 102ms/step - loss: 0.0602 - accuracy: 0.9873 - val_loss: 0.0632 - val_accuracy: 0.9821\n",
      "Epoch 4/5\n",
      "280/280 [==============================] - 28s 101ms/step - loss: 0.0294 - accuracy: 0.9933 - val_loss: 0.0538 - val_accuracy: 0.9835\n",
      "Epoch 5/5\n",
      "280/280 [==============================] - 29s 102ms/step - loss: 0.0194 - accuracy: 0.9952 - val_loss: 0.0505 - val_accuracy: 0.9842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2200d17dde0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 6. Model Training ===\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=5, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0505 - accuracy: 0.9842\n",
      "Test Accuracy: 0.9842\n"
     ]
    }
   ],
   "source": [
    "# === 7. Testing ===\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
