{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqXoek6foX8O"
   },
   "source": [
    "# Lab 3: Generative Models (Denoising Diffusion Probabilistic Model)\n",
    "```\n",
    "- [S25] Advanced Machine Learning, Innopolis University\n",
    "- Teaching Assistant: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "```\n",
    "Lab Plan\n",
    "\n",
    "In this lab we will try to build a very simple (as few code as possible) Diffusion Model for generating images.\n",
    "\n",
    "1. The forward process\n",
    "2. The backward process = U-Net\n",
    "3. The Training procedure\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-oUK3BsE022"
   },
   "source": [
    "## Recap\n",
    "\n",
    "What have we covered so far and how is it linked to Diffusion Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "803IkQ87Dxf2"
   },
   "source": [
    "<!-- ![](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/generative-overview.png) -->\n",
    "<img src='https://lilianweng.github.io/posts/2021-07-11-diffusion-models/generative-overview.png' width=\"100%\" height=\"100%\"/>\n",
    "\n",
    "<!-- <table><tr>\n",
    "<td> <img src='https://lilianweng.github.io/posts/2021-07-11-diffusion-models/generative-overview.png' style=\"width:30%\"/> </td>\n",
    "<td> <img src='https://lilianweng.github.io/posts/2021-07-11-diffusion-models/generative-overview.png' style=\"width:30%\"/> </td>\n",
    "</tr></table> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gWWs5tMFylD"
   },
   "source": [
    "## Denoising Diffusion Probabilistic Model\n",
    "\n",
    "**Overall Idea:**<br>\n",
    "**Original Publication:** <br>\n",
    "**Application & Usage:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utW_MZmhEcHF"
   },
   "source": [
    "![](https://learnopencv.com/wp-content/uploads/2023/02/denoising-diffusion-probabilistic-models-forward_and_backward_equations.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8wlOcKeaBQyj"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdL5jv9QBTxY"
   },
   "source": [
    "## Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxZ11LX6AvGy",
    "outputId": "073b84d1-8ca7-4532-bb35-7c13f12579c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "img_size = (28, 28, 1) # (width, height, channels)\n",
    "\n",
    "timestep_embedding_dim = 256\n",
    "n_layers = 8\n",
    "hidden_dim = 256\n",
    "n_timesteps = 1000\n",
    "beta_minmax=[1e-4, 2e-2]\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "hidden_dims = [hidden_dim for _ in range(n_layers)]\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zK4v2Ou1H-Rp"
   },
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HFkOdCEPIBoI"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "train_dataset = MNIST('./data', transform=transform, train=True, download=True)\n",
    "test_dataset  = MNIST('./data', transform=transform, train=False, download=True)\n",
    "\n",
    "train_batch_size = 128\n",
    "inference_batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(dataset=test_dataset,  batch_size=inference_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpuswzFfGlOa"
   },
   "source": [
    "## Implementation ✨\n",
    "\n",
    "* **Step 1**: Define Foward diffusion\n",
    "* **Step 2**: Define Reverse Diffusion Process\n",
    "* **Step 3:** Train Denoising Diffusion Probabilistic Model (DDPM)\n",
    "\n",
    "**Note:** Implementation closer to original Denoising Diffusion Probabilistic Model paper but with some modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ht0KVo65HwBa"
   },
   "source": [
    "### Step 1: The forward process = Noise scheduler\n",
    "\n",
    "![](https://learnopencv.com/wp-content/uploads/2023/01/diffusion-models-forwardbackward_process_ddpm.png)\n",
    "\n",
    "We first need to build the inputs for our model, which are more and more noisy images. Instead of doing this sequentially, we can use the closed form provided in the papers to calculate the image for any of the timesteps individually.\n",
    "\n",
    "**Key Takeaways**:\n",
    "- The noise-levels/variances can be pre-computed\n",
    "- There are different types of variance schedules\n",
    "- We can sample each timestep image independently (Sums of Gaussians is also Gaussian)\n",
    "- No model is needed in this forward step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gybxcfO9oUqs"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    \"\"\"\n",
    "    Returns a specific index t of a passed list of values vals\n",
    "    while considering the batch dimension.\n",
    "    \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Takes an image and a timestep as input and\n",
    "    returns the noisy version of it\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
    "    )\n",
    "    # mean + variance\n",
    "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \\\n",
    "    + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)\n",
    "\n",
    "\n",
    "# Define beta schedule\n",
    "T = 300\n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "\n",
    "# Pre-calculate different terms for closed form\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1ug6BWRYDIp"
   },
   "source": [
    "#### Visialize the foward diffusion process\n",
    "\n",
    "**Lab TASK**: Implement a function that takes an image represented as a tensor and visualizes it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nNkd0qY5Xd4j"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_tensor_image(image_tensor):\n",
    "    \"\"\"Visualize a tensor as an image\"\"\"\n",
    "    \n",
    "    if image_tensor.ndimension() == 4:  \n",
    "        image_tensor = image_tensor[0]\n",
    "    \n",
    "    image_numpy = image_tensor.squeeze().cpu().numpy()  \n",
    "    \n",
    "    if image_numpy.ndimension() == 2:\n",
    "        plt.imshow(image_numpy, cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(image_numpy)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YovzvX4EXPqZ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_tensor_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# plt.subplot(1, num_images+1, int(idx/stepsize) + 1)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m img, noise \u001b[38;5;241m=\u001b[39m forward_diffusion_sample(image, t)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mshow_tensor_image\u001b[49m(img)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'show_tensor_image' is not defined"
     ]
    }
   ],
   "source": [
    "# Simulate forward diffusion\n",
    "image = next(iter(train_loader))[0]\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.axis('off')\n",
    "num_images = 10\n",
    "stepsize = int(T/num_images)\n",
    "\n",
    "for idx in range(0, T, stepsize):\n",
    "    t = torch.Tensor([idx]).type(torch.int64)\n",
    "    plt.subplot(1, num_images+1, int(idx/stepsize) + 1)\n",
    "    img, noise = forward_diffusion_sample(image, t)\n",
    "    show_tensor_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Tkxd2sZJTHt"
   },
   "source": [
    "### Step 2: The backward process\n",
    "\n",
    "![](https://learnopencv.com/wp-content/uploads/2023/02/denoising-diffusion-probabilistic-models-overall_reverse_diffusion_process-1024x173.png)\n",
    "\n",
    "In original paper U-net is used to facilitate the backward/reverse process.\n",
    "For a great introduction to UNets, have a look at this post: https://amaarora.github.io/2020/09/13/unet.html.\n",
    "\n",
    "\n",
    "**Key Notes**:\n",
    "- We use a simple stacked-convolution model with various dilations instead of UNet-like architecture to predict the noise in the image\n",
    "- The input is a noisy image, the ouput the noise in the image\n",
    "- Because the parameters are shared accross time, we need to tell the network in which timestep we are\n",
    "- The Timestep is encoded by the transformer Sinusoidal Embedding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8nFEpFrPIjo"
   },
   "source": [
    "#### Sinusoidal embedding for diffusion timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aWdH0xb-PQcT"
   },
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "  def __init__(self, dim):\n",
    "    super().__init__()\n",
    "    self.dim = dim\n",
    "\n",
    "  def forward(self, x):\n",
    "    device = x.device\n",
    "    half_dim = self.dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "    emb = x[:, None] * emb[None, :]\n",
    "    emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyOQZr_YPZmF"
   },
   "source": [
    "#### Stacked-convolution model (instead of U-net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Qv-heWe4qX18"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, activation_fn=None, drop_rate=0., stride=1,\n",
    "                 padding='same', dilation=1, groups=1, bias=True, gn=False, gn_groups=8):\n",
    "\n",
    "        if padding == 'same':\n",
    "            padding = kernel_size // 2 * dilation\n",
    "\n",
    "        super(ConvBlock, self).__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.activation_fn = nn.SiLU() if activation_fn else None\n",
    "        self.group_norm = nn.GroupNorm(gn_groups, out_channels) if gn else None\n",
    "\n",
    "    def forward(self, x, time_embedding=None, residual=False):\n",
    "\n",
    "        if residual:\n",
    "            # in the paper, diffusion timestep embedding was only applied to residual blocks of U-Net\n",
    "            x = x + time_embedding\n",
    "            y = x\n",
    "            x = super(ConvBlock, self).forward(x)\n",
    "            y = y + x\n",
    "        else:\n",
    "            y = super(ConvBlock, self).forward(x)\n",
    "        y = self.group_norm(y) if self.group_norm is not None else y\n",
    "        y = self.activation_fn(y) if self.activation_fn is not None else y\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PnAB4VCdPieb"
   },
   "outputs": [],
   "source": [
    "class Denoiser(nn.Module):\n",
    "\n",
    "    def __init__(self, image_resolution, hidden_dims=[256, 256], diffusion_time_embedding_dim = 256, n_times=1000):\n",
    "        super(Denoiser, self).__init__()\n",
    "\n",
    "        _, _, img_C = image_resolution\n",
    "\n",
    "        self.time_embedding = SinusoidalPosEmb(diffusion_time_embedding_dim)\n",
    "        self.in_project = ConvBlock(img_C, hidden_dims[0], kernel_size=7)\n",
    "        self.time_project = nn.Sequential(\n",
    "                                 ConvBlock(diffusion_time_embedding_dim, hidden_dims[0], kernel_size=1, activation_fn=True),\n",
    "                                 ConvBlock(hidden_dims[0], hidden_dims[0], kernel_size=1))\n",
    "\n",
    "        self.convs = nn.ModuleList([ConvBlock(in_channels=hidden_dims[0], out_channels=hidden_dims[0], kernel_size=3)])\n",
    "\n",
    "        for idx in range(1, len(hidden_dims)):\n",
    "            self.convs.append(ConvBlock(hidden_dims[idx-1], hidden_dims[idx], kernel_size=3, dilation=3**((idx-1)//2),\n",
    "                                        activation_fn=True, gn=True, gn_groups=8))\n",
    "\n",
    "        self.out_project = ConvBlock(hidden_dims[-1], out_channels=img_C, kernel_size=3)\n",
    "\n",
    "\n",
    "    def forward(self, perturbed_x, diffusion_timestep):\n",
    "        y = perturbed_x\n",
    "\n",
    "        diffusion_embedding = self.time_embedding(diffusion_timestep)\n",
    "        diffusion_embedding = self.time_project(diffusion_embedding.unsqueeze(-1).unsqueeze(-2))\n",
    "\n",
    "        y = self.in_project(y)\n",
    "\n",
    "        for i in range(len(self.convs)):\n",
    "            y = self.convs[i](y, diffusion_embedding, residual = True)\n",
    "\n",
    "        y = self.out_project(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "denoiser_model = Denoiser(image_resolution=img_size,\n",
    "                          hidden_dims=hidden_dims,\n",
    "                          diffusion_time_embedding_dim=timestep_embedding_dim,\n",
    "                          n_times=n_timesteps).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NOx3H5ngKQ34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Denoiser(\n",
       "  (time_embedding): SinusoidalPosEmb()\n",
       "  (in_project): ConvBlock(1, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "  (time_project): Sequential(\n",
       "    (0): ConvBlock(\n",
       "      256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
       "      (activation_fn): SiLU()\n",
       "    )\n",
       "    (1): ConvBlock(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (convs): ModuleList(\n",
       "    (0): ConvBlock(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ConvBlock(\n",
       "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "      (activation_fn): SiLU()\n",
       "      (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (2): ConvBlock(\n",
       "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "      (activation_fn): SiLU()\n",
       "      (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (3): ConvBlock(\n",
       "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3)\n",
       "      (activation_fn): SiLU()\n",
       "      (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (4): ConvBlock(\n",
       "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3)\n",
       "      (activation_fn): SiLU()\n",
       "      (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (5): ConvBlock(\n",
       "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(9, 9), dilation=(9, 9)\n",
       "      (activation_fn): SiLU()\n",
       "      (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (6): ConvBlock(\n",
       "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(9, 9), dilation=(9, 9)\n",
       "      (activation_fn): SiLU()\n",
       "      (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (7): ConvBlock(\n",
       "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(27, 27), dilation=(27, 27)\n",
       "      (activation_fn): SiLU()\n",
       "      (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (out_project): ConvBlock(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoiser_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axukUnLeQmgb"
   },
   "source": [
    "## Full Diffusion Model (foward + reverse process)\n",
    "![](https://i.imgur.com/S7KH5hZ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OcYWwAkKQm-9"
   },
   "outputs": [],
   "source": [
    "class Diffusion(nn.Module):\n",
    "    def __init__(self, model, image_resolution=[32, 32, 3], n_times=1000, beta_minmax=[1e-4, 2e-2], device='cuda'):\n",
    "\n",
    "        super(Diffusion, self).__init__()\n",
    "\n",
    "        self.n_times = n_times\n",
    "        self.img_H, self.img_W, self.img_C = image_resolution\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        # define linear variance schedule(betas)\n",
    "        beta_1, beta_T = beta_minmax\n",
    "        betas = torch.linspace(start=beta_1, end=beta_T, steps=n_times).to(device) # follows DDPM paper\n",
    "        self.sqrt_betas = torch.sqrt(betas)\n",
    "\n",
    "        # define alpha for forward diffusion kernel\n",
    "        self.alphas = 1 - betas\n",
    "        self.sqrt_alphas = torch.sqrt(self.alphas)\n",
    "        alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "        self.sqrt_one_minus_alpha_bars = torch.sqrt(1-alpha_bars)\n",
    "        self.sqrt_alpha_bars = torch.sqrt(alpha_bars)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def extract(self, a, t, x_shape):\n",
    "        b, *_ = t.shape\n",
    "        out = a.gather(-1, t)\n",
    "        return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "    def scale_to_minus_one_to_one(self, x):\n",
    "        # according to the DDPMs paper, normalization seems to be crucial to train reverse process network\n",
    "        return x * 2 - 1\n",
    "\n",
    "    def reverse_scale_to_zero_to_one(self, x):\n",
    "        return (x + 1) * 0.5\n",
    "\n",
    "    def make_noisy(self, x_zeros, t):\n",
    "        # perturb x_0 into x_t (i.e., take x_0 samples into forward diffusion kernels)\n",
    "        epsilon = torch.randn_like(x_zeros).to(self.device)\n",
    "\n",
    "        sqrt_alpha_bar = self.extract(self.sqrt_alpha_bars, t, x_zeros.shape)\n",
    "        sqrt_one_minus_alpha_bar = self.extract(self.sqrt_one_minus_alpha_bars, t, x_zeros.shape)\n",
    "        noisy_sample = x_zeros * sqrt_alpha_bar + epsilon * sqrt_one_minus_alpha_bar\n",
    "\n",
    "        return noisy_sample.detach(), epsilon\n",
    "\n",
    "\n",
    "    def forward(self, x_zeros):\n",
    "        x_zeros = self.scale_to_minus_one_to_one(x_zeros)\n",
    "\n",
    "        B, _, _, _ = x_zeros.shape\n",
    "\n",
    "        # (1) randomly choose diffusion time-step\n",
    "        t = torch.randint(low=0, high=self.n_times, size=(B,)).long().to(self.device)\n",
    "\n",
    "        # (2) forward diffusion process: perturb x_zeros with fixed variance schedule\n",
    "        perturbed_images, epsilon = self.make_noisy(x_zeros, t)\n",
    "\n",
    "        # (3) predict epsilon/noise given perturbed data at diffusion-timestep t.\n",
    "        pred_epsilon = self.model(perturbed_images, t)\n",
    "\n",
    "        return perturbed_images, epsilon, pred_epsilon\n",
    "\n",
    "\n",
    "    def denoise_at_t(self, x_t, timestep, t):\n",
    "        B, _, _, _ = x_t.shape\n",
    "        if t > 1:\n",
    "            z = torch.randn_like(x_t).to(self.device)\n",
    "        else:\n",
    "            z = torch.zeros_like(x_t).to(self.device)\n",
    "\n",
    "        # at inference, we use predicted noise / epsilon to restore perturbed data sample.\n",
    "        epsilon_pred = self.model(x_t, timestep)\n",
    "\n",
    "        alpha = self.extract(self.alphas, timestep, x_t.shape)\n",
    "        sqrt_alpha = self.extract(self.sqrt_alphas, timestep, x_t.shape)\n",
    "        sqrt_one_minus_alpha_bar = self.extract(self.sqrt_one_minus_alpha_bars, timestep, x_t.shape)\n",
    "        sqrt_beta = self.extract(self.sqrt_betas, timestep, x_t.shape)\n",
    "\n",
    "        # denoise at time t, utilizing predicted noise\n",
    "        x_t_minus_1 = 1 / sqrt_alpha * (x_t - (1-alpha)/sqrt_one_minus_alpha_bar*epsilon_pred) + sqrt_beta*z\n",
    "\n",
    "        return x_t_minus_1.clamp(-1., 1)\n",
    "\n",
    "    def sample(self, N):\n",
    "        # start from random noise vector, x_0 (for simplicity, x_T declared as x_t instead of x_T)\n",
    "        x_t = torch.randn((N, self.img_C, self.img_H, self.img_W)).to(self.device)\n",
    "\n",
    "        # autoregressively denoise from x_T to x_0\n",
    "        for t in range(self.n_times-1, -1, -1):\n",
    "            timestep = torch.tensor([t]).repeat_interleave(N, dim=0).long().to(self.device)\n",
    "            x_t = self.denoise_at_t(x_t, timestep, t)\n",
    "\n",
    "        # denormalize x_0 into 0 ~ 1 ranged values.\n",
    "        x_0 = self.reverse_scale_to_zero_to_one(x_t)\n",
    "\n",
    "        return x_0\n",
    "\n",
    "\n",
    "diffusion_model = Diffusion(denoiser_model, image_resolution=img_size, n_times=n_timesteps,\n",
    "                            beta_minmax=beta_minmax, device=device).to(device)\n",
    "lr = 5e-5\n",
    "optimizer = Adam(diffusion_model.parameters(), lr=lr)\n",
    "denoising_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCOo4jCCKd91",
    "outputId": "d5d37345-b4d4-4ce4-c054-d1a5162e722f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Diffusion(\n",
       "  (model): Denoiser(\n",
       "    (time_embedding): SinusoidalPosEmb()\n",
       "    (in_project): ConvBlock(1, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (time_project): Sequential(\n",
       "      (0): ConvBlock(\n",
       "        256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (activation_fn): SiLU()\n",
       "      )\n",
       "      (1): ConvBlock(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (convs): ModuleList(\n",
       "      (0): ConvBlock(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ConvBlock(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (activation_fn): SiLU()\n",
       "        (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (2): ConvBlock(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (activation_fn): SiLU()\n",
       "        (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (3): ConvBlock(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3)\n",
       "        (activation_fn): SiLU()\n",
       "        (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (4): ConvBlock(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3)\n",
       "        (activation_fn): SiLU()\n",
       "        (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (5): ConvBlock(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(9, 9), dilation=(9, 9)\n",
       "        (activation_fn): SiLU()\n",
       "        (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (6): ConvBlock(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(9, 9), dilation=(9, 9)\n",
       "        (activation_fn): SiLU()\n",
       "        (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (7): ConvBlock(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(27, 27), dilation=(27, 27)\n",
       "        (activation_fn): SiLU()\n",
       "        (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (out_project): ConvBlock(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8M-oVnFTEly"
   },
   "source": [
    "## Step 3. Train Denoising Diffusion Probabilistic Models(DDPMs)\n",
    "![](https://huggingface.co/blog/assets/78_annotated-diffusion/training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XwoXJ3_iTetU",
    "outputId": "d8c58e4d-4093-4cc8-d3fe-eca1b373a641"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [05:50<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[Epoch 1 ]\tDenoising Loss:  0.08883268748306566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [05:48<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[Epoch 2 ]\tDenoising Loss:  0.03889823886446464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 53/469 [00:40<05:21,  1.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m noisy_input, epsilon, pred_epsilon \u001b[38;5;241m=\u001b[39m diffusion_model(x)\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m denoising_loss(pred_epsilon, epsilon)\n\u001b[1;32m---> 14\u001b[0m noise_prediction_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "denoiser_model.train()\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    noise_prediction_loss = 0\n",
    "    for batch_idx, (x, _) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = x.to(device)\n",
    "\n",
    "        noisy_input, epsilon, pred_epsilon = diffusion_model(x)\n",
    "        loss = denoising_loss(pred_epsilon, epsilon)\n",
    "\n",
    "        noise_prediction_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"\\t[Epoch\", epoch + 1, \"]\\tDenoising Loss: \", noise_prediction_loss / batch_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2aTMGi5P9hx"
   },
   "source": [
    "## Results visualization and Generating Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2uJ1CJ_PhdL"
   },
   "outputs": [],
   "source": [
    "# Take one batch from testset\n",
    "denoiser_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_images = diffusion_model.sample(N=10)\n",
    "\n",
    "def show_image(x, idx):\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(x[idx].transpose(0, 1).transpose(1, 2).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "xExL9ZRsQYCP",
    "outputId": "1d3b2a0a-345e-47dc-8911-c79aaadba5dd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(generated_images, idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKwpFrCBA1DO"
   },
   "source": [
    "# Resources\n",
    "- Github implementation [Denoising Diffusion Pytorch](https://github.com/lucidrains/denoising-diffusion-pytorch)\n",
    "- Niels Rogge, Kashif Rasul, [Huggingface notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb#scrollTo=3a159023)\n",
    "- Papers on Diffusion models ([Dhariwal, Nichol, 2021], [Ho et al., 2020] ect.)\n",
    "- [An In-Depth Guide to Denoising Diffusion Probabilistic Models - From Theory to Implementation](https://learnopencv.com/denoising-diffusion-probabilistic-models/)\n",
    "- [A Survey on Generative Diffusion Model](https://arxiv.org/pdf/2209.02646.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2xlrqy2A3oO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
