{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGHohjVDJq2m"
   },
   "source": [
    "# Reinforcement Learning\n",
    "* what is RL?\n",
    "* Does it have to do anything with Machine Learning?\n",
    "* Reinforcement learing is a branch of machine learning, But what makes it a branch?\n",
    " * There is no **supervisor**, only a <font color='skyblue'> **Reward**</font> signal\n",
    "* RL is active, Not passive!\n",
    "* Interactions are often squential\n",
    "* you have the <font color='skyblue'> **Agent** </font> that interacts with the <font color='skyblue'> **Environment** </font>\n",
    "* Agent takes an <font color='skyblue'> **Action** </font> and in return gets **reward** and a new <font color='skyblue'> **State** </font>\n",
    "* **Goal** is to select sequence of actions that will **maximize** the reward\n",
    "* In some cases reward can be **delayed**\n",
    "* We need to think <font color=\"lightgreen\"> **long-term** </font> instead of <font color=\"red\"> **short-term** </font> in order to achive our goal\n",
    "* But how do we know from which action we can benefit the most?\n",
    " * **Policy**: a strategy that an agent uses in pursue it's goal\n",
    "* We need some kind of function to refer in order to make desicion.\n",
    "<!--\n",
    "![RL cycle.png](https://drive.google.com/uc?id=1o0gGJQEr1YUUkeaK5OzOzKIodJAV326Y) -->\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJGHZoItaiSD"
   },
   "source": [
    "# [Knowledge Check](https://github.com/huggingface/deep-rl-class/blob/main/unit1/quiz.md) ✔️\n",
    "\n",
    "The best way to learn and [to avoid the illusion of competence](https://fr.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf) **is to test yourself.** This will help you to find **where you need to reinforce your knowledge**.\n",
    "\n",
    "\n",
    "### Q1: What is Reinforcement Learning?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "  \n",
    "Reinforcement learning is a **framework for solving control tasks (also called decision problems)** by building agents that learn from the environment by interacting with it through trial and error and **receiving rewards (positive or negative) as unique feedback**.\n",
    "  \n",
    "</details>\n",
    "\n",
    "### Q2: Define the RL Loop\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1JeSNuj-ZjKBageYbPr-4IqQaz0edU7KF\" alt=\"Exercise RL Loop\"/>\n",
    "\n",
    "At every step:\n",
    "- Our Agent receives ______ from the environment\n",
    "- Based on that ______ the Agent takes an ______\n",
    "- Our Agent will move to the right\n",
    "- The Environment goes to a ______\n",
    "- The Environment gives ______ to the Agent\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1r58UXRw4IUy9HohiwTsCpp8IPYawckr1\" alt=\"Exercise RL Solution\"/>\n",
    "  \n",
    "\n",
    "At every step:\n",
    "- Our Agent receives **state s0** from the environment\n",
    "- Based on that **state s0** the Agent takes an **action a0**\n",
    "- Our Agent will move to the right\n",
    "- The Environment goes to a **new state s1**\n",
    "- The Environment gives **a reward r1** to the Agent\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "### Q3: What's the difference between a state and an observation?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "  \n",
    "- *The state* is a **complete description of the state of the world** (there is no hidden information), in a fully observed environment. For instance, in chess game, we receive a state from the environment since we have access to the whole checkboard information.\n",
    "  \n",
    "- *The observation* is a **partial description of the state**. In a partially observed environment. For instance, in Super Mario Bros, we only see a part of the level close to the player, so we receive an observation.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1IKGZikXTLOLHxR6qhW73Wf5qxQTifqrs\" alt=\"Observation Space Recap\"/>  \n",
    "\n",
    "</details>\n",
    "\n",
    "### Q4: A task is an instance of a Reinforcement Learning problem. What are the two types of tasks?\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "  \n",
    "- *Episodic task* : we have a **starting point and an ending point (a terminal state)**. This creates an episode: a list of States, Actions, Rewards, and new States. For instance, think about Super Mario Bros: an episode begin at the launch of a new Mario Level and ending when you’re killed or you reached the end of the level.\n",
    "  \n",
    "- *Continuous task* : these are tasks that **continue forever (no terminal state)**. In this case, the agent must learn how to choose the best actions and simultaneously interact with the environment.\n",
    "  \n",
    "<img src=\"https://drive.google.com/uc?id=1XEENs8_w0sYrQavLLY4Fhok1hKc5-Muu\" alt=\"Task\"/>  \n",
    "\n",
    "</details>\n",
    "\n",
    "### Q5: What is the exploration/exploitation tradeoff?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "  \n",
    "In Reinforcement Learning, we need to **balance how much we explore the environment and how much we exploit what we know about the environment**.\n",
    "\n",
    "- *Exploration* is exploring the environment by **trying random actions in order to find more information about the environment**.\n",
    "\n",
    "- *Exploitation* is **exploiting known information to maximize the reward**.\n",
    "  \n",
    "<img src=\"https://drive.google.com/uc?id=1033zZ24LNyz2RykwRfKDDaVE9b3FhejM\" alt=\"Exploration/exploitation tradeoff\"/>  \n",
    "\n",
    "</details>\n",
    "\n",
    "### Q6: What is a policy?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "  \n",
    "- The Policy π **is the brain of our Agent**, it’s the function that tell us what action to take given the state we are. So it defines the agent’s behavior at a given time.\n",
    "  \n",
    "<img src=\"https://drive.google.com/uc?id=1V1HH5PHdjP_O1QZqOowR0H3klN0aw1-f\" alt=\"Policy\"/>  \n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "### Q7: What are value-based methods?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "  \n",
    "- Value-based methods is one of the main approaches for solving RL problems.\n",
    "- In Value-based methods, instead of training a policy function, **we train a value function that maps a state to the expected value of being at that state**.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=19__sJ1-EGl71Mcxh47UNqWanFVpxkHd_\" alt=\"Value illustration\"/>\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q8: What are policy-based methods?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "  \n",
    "- In *Policy-Based Methods*, we learn a **policy function directly**.\n",
    "- This policy function will **map from each state to the best corresponding action at that state**. Or a **probability distribution over the set of possible actions at that state**.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1V1HH5PHdjP_O1QZqOowR0H3klN0aw1-f\" alt=\"Policy illustration\"/>\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_A38vTNQFcw"
   },
   "source": [
    "## Use case:\n",
    "#### Atari games:\n",
    "[Click here](https://www.youtube.com/watch?v=V1eYniJ0Rnk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jUICg3lxDZJ"
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dluW78iEaf_C"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "if os.path.exists('.setup_complete'):\n",
    "    !pip install gym[all]\n",
    "    clear_output()\n",
    "\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/mhd-medfa/IU-Reinforcement-Learning-22-lab/main/setup_colab.sh -O- | bash\n",
    "    !touch .setup_complete\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cLnfMY7BVOHz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyglet==1.5.27\n",
      "  Downloading pyglet-1.5.27-py3-none-any.whl.metadata (7.6 kB)\n",
      "Downloading pyglet-1.5.27-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.0/1.1 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 4.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pyglet\n",
      "  Attempting uninstall: pyglet\n",
      "    Found existing installation: pyglet 1.5.1\n",
      "    Uninstalling pyglet-1.5.1:\n",
      "      Successfully uninstalled pyglet-1.5.1\n",
      "Successfully installed pyglet-1.5.27\n"
     ]
    }
   ],
   "source": [
    "!pip install pyglet==1.5.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mwkde8s5gh3h"
   },
   "source": [
    "### ***Re-run*** previous cell ☝ before you continue!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIavzcg6aDD3"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqFuyzvrsUGN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bM_UmUD-l4W5"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCYdai6TxOc7"
   },
   "source": [
    "# Load Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEUZwKXEsqj5"
   },
   "source": [
    "### OpenAI Gym\n",
    "\n",
    "We're gonna spend several next weeks learning algorithms that solve decision processes. We are then in need of some interesting decision problems to test our algorithms.\n",
    "\n",
    "That's where OpenAI Gym comes into play. It's a Python library that wraps many classical decision problems including robot control, videogames and board games.\n",
    "\n",
    "So here's and example that shows how it works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzIK2IdonVGy"
   },
   "source": [
    "## Cartpole-V0\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1Qpj_jmy3AUqxieDRmWqY8PLkemAZ2jcx\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-ugxR1-xYNK"
   },
   "outputs": [],
   "source": [
    "environment_name = \"CartPole-v1\"\n",
    "env = gym.make(environment_name, render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fRONrNfEDJL"
   },
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0kmc2GlvHyC"
   },
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info, _ = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQ2cs0A0x39o"
   },
   "source": [
    "# Understanding The Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EZEONTqs2Kn"
   },
   "source": [
    "**CartPole-v0 Description:**\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's\n",
    "velocity.\n",
    "        \n",
    "**Starting State:**\n",
    "\n",
    "      All observations are assigned a uniform random value in [-0.05, 0.05]\n",
    "        \n",
    "**Actions:**\n",
    "\n",
    "    Type: Discrete(2)\n",
    "\n",
    "    | Num |         Action         |\n",
    "    |-----|------------------------|\n",
    "    |  0  | Push cart to the left  |\n",
    "    |  1  | Push cart to the right |\n",
    "\n",
    "    Note: The amount the velocity that is reduced or increased is not\n",
    "    fixed; it depends on the angle the pole is pointing. This is because\n",
    "    the center of gravity of the pole increases the amount of energy needed\n",
    "    to move the cart underneath it\n",
    "        \n",
    "**Observation:**\n",
    "      \n",
    "      [position of cart, velocity of cart, angle of pole, rotation rate of pole]\n",
    "\n",
    "**Episode Termination:**\n",
    "\n",
    "            Pole Angle is more than 12 degrees.\n",
    "            Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "            the display).\n",
    "            Episode length is greater than 500 (200 for v0).\n",
    "            Solved Requirements:\n",
    "            Considered solved when the average return is greater than or equal to\n",
    "            495.0 (195.0 for v0) over 100 consecutive trials.\n",
    "\n",
    "\n",
    "Ref: https://gymnasium.farama.org/environments/classic_control/cart_pole/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzaEkg-8svhl"
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "plt.imshow(env.render())\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwpLXdT6vG9H"
   },
   "source": [
    "Note: if you're running this on your local machine, you'll see a window pop up with the image above. Don't close it, just alt-tab away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSt_kqHRywz6"
   },
   "outputs": [],
   "source": [
    "# 0-push cart to left, 1-push cart to the right\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7QxXiosy33-"
   },
   "outputs": [],
   "source": [
    "# [cart position, cart velocity, pole angle, pole angular velocity]\n",
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQy0phR3gyvM"
   },
   "source": [
    "# Train your first RL Model\n",
    "\n",
    "[Stable Baselines3](https://github.com/DLR-RM/stable-baselines3) (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of Stable Baselines.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This [table](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html) displays the rl algorithms that are implemented in the Stable Baselines3 project, along with some useful characteristics: support for discrete/continuous actions, multiprocessing.\n",
    "\n",
    "<div class=\"wy-table-responsive\"><table class=\"docutils align-default\">\n",
    "<colgroup>\n",
    "<col style=\"width: 21%\">\n",
    "<col style=\"width: 12%\">\n",
    "<col style=\"width: 13%\">\n",
    "<col style=\"width: 19%\">\n",
    "<col style=\"width: 17%\">\n",
    "<col style=\"width: 18%\">\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"row-odd\"><th class=\"head\"><p>Name</p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">Box</span></code></p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">Discrete</span></code></p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">MultiDiscrete</span></code></p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">MultiBinary</span></code></p></th>\n",
    "<th class=\"head\"><p>Multi Processing</p></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"row-even\"><td><p>ARS </p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>A2C</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>DDPG</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>DQN</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>HER</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>PPO</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>QR-DQN </p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>️ ✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>RecurrentPPO </p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>SAC</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>TD3</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>TQC </p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>TRPO  </p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Maskable PPO </p></td>\n",
    "<td><p>❌</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table></div>\n",
    "\n",
    "\n",
    "<p>Vectorized Environments are a method for stacking multiple independent environments into a single environment.\n",
    "Instead of training an RL agent on 1 environment per step, it allows us to train it on <code class=\"docutils literal notranslate\"><span class=\"pre\">n</span></code> environments per step.\n",
    "Because of this, <code class=\"docutils literal notranslate\"><span class=\"pre\">actions</span></code> passed to the environment are now a vector (of dimension <code class=\"docutils literal notranslate\"><span class=\"pre\">n</span></code>).\n",
    "It is the same for <code class=\"docutils literal notranslate\"><span class=\"pre\">observations</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">rewards</span></code> and end of episode signals (<code class=\"docutils literal notranslate\"><span class=\"pre\">dones</span></code>).\n",
    "In the case of non-array observation spaces such as <code class=\"docutils literal notranslate\"><span class=\"pre\">Dict</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">Tuple</span></code>, where different sub-spaces\n",
    "may have different shapes, the sub-observations are vectors (of dimension <code class=\"docutils literal notranslate\"><span class=\"pre\">n</span></code>).</p>\n",
    "\n",
    "\n",
    "<div class=\"wy-table-responsive\"><table class=\"docutils align-default\">\n",
    "<colgroup>\n",
    "<col style=\"width: 20%\">\n",
    "<col style=\"width: 11%\">\n",
    "<col style=\"width: 18%\">\n",
    "<col style=\"width: 12%\">\n",
    "<col style=\"width: 14%\">\n",
    "<col style=\"width: 25%\">\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"row-odd\"><th class=\"head\"><p>Name</p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">Box</span></code></p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">Discrete</span></code></p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">Dict</span></code></p></th>\n",
    "<th class=\"head\"><p><code class=\"docutils literal notranslate\"><span class=\"pre\">Tuple</span></code></p></th>\n",
    "<th class=\"head\"><p>Multi Processing</p></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"row-even\"><td><p>DummyVecEnv</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>❌️</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>SubprocVecEnv</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "<td><p>✔️</p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table></div>\n",
    "\n",
    "Ref:\n",
    "* https://stable-baselines3.readthedocs.io/en/master/guide/algos.html\n",
    "* https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fY7xZbytEna"
   },
   "outputs": [],
   "source": [
    "model_path = './models'\n",
    "log_path = './logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZsZFnc-umxQ"
   },
   "outputs": [],
   "source": [
    "environment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rp5HyzMFgyvM"
   },
   "outputs": [],
   "source": [
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZ1lJpeJgyvN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c0IH9EugyvN"
   },
   "source": [
    "# Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjYHLcR3gyvN"
   },
   "outputs": [],
   "source": [
    "PPO_path = os.path.join(model_path, 'PPO_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1rstWIggyvN"
   },
   "outputs": [],
   "source": [
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pQBK_R8gyvN"
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bb2veOhnNKO"
   },
   "outputs": [],
   "source": [
    "model = PPO.load(PPO_path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42RJ_A17gyvO"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2RbAkBVagyvO"
   },
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpP745rDgyvP"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sz6e3U8EgyvP"
   },
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaUBqsVdgyvP"
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    # action = env.action_space.sample()\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    if done:\n",
    "        print('info', info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UW4C4fGVgyvP"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3neaxW8bioX"
   },
   "source": [
    "## Atari Pong:\n",
    "*Pong* is an old Atari game which have inspired from table tennis aka PingPong. Pong is a simple game that the player only have 3 actions (Up, Down, Idle). Since the game is simple and the behaviour of the environment is the limited we can program an agent with classical algorithms.\n",
    "\n",
    "A naive classic algorithm: If the *y* coordinate of the ball is higher than *y* coordinate of the player. choose action *UP*. If *y* coordinate of the ball is lower than the player choose action *DOWN*.\n",
    "\n",
    "![gif of how the player can play against human]()\n",
    "\n",
    " Using RL we want to train the and **Agent**. We want a model that can outrace the classical algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7mIKrdubzEh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
