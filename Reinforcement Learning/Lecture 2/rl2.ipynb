{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i-UF44JQuuW"
   },
   "source": [
    "Open this [link](https://rl-lab.com/multi-armed-bandits/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Kgc_nCo10vHN"
   },
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod, abstractproperty\n",
    "import enum\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import pandas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hzpz05vH0vHP"
   },
   "source": [
    "## Part 1. Bernoulli Bandit\n",
    "\n",
    "We are going to implement several exploration strategies for simplest problem - bernoulli bandit.\n",
    "\n",
    "The bandit has $K$ actions. Action produce 1.0 reward $r$ with probability $0 \\le \\theta_k \\le 1$ which is unknown to agent, but fixed over time. Agent's objective is to minimize regret over fixed number $T$ of action selections:\n",
    "\n",
    "$$\\rho = T\\theta^* - \\sum_{t=1}^T r_t$$\n",
    "\n",
    "Where $\\theta^* = \\max_k\\{\\theta_k\\}$\n",
    "\n",
    "**Real-world analogy:**\n",
    "\n",
    "Clinical trials - we have $K$ pills and $T$ ill patient. After taking pill, patient is cured with probability $\\theta_k$. Task is to find most efficient pill.\n",
    "\n",
    "A research on clinical trials - https://arxiv.org/pdf/1507.08025.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "m0p3R44Z0vHQ"
   },
   "outputs": [],
   "source": [
    "class BernoulliBandit:\n",
    "  def __init__(self, n_actions=5):\n",
    "    self._probs = np.random.random(n_actions)\n",
    "\n",
    "  @property\n",
    "  def action_count(self):\n",
    "    return len(self._probs)\n",
    "\n",
    "  def pull(self, action):\n",
    "    if np.random.random() > self._probs[action]:\n",
    "      return 0.0\n",
    "    else: return 1.0\n",
    "  def optimal_reward(self):\n",
    "    return np.max(self._probs)\n",
    "\n",
    "  def reset(self):\n",
    "    pass\n",
    "  def step(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BzQGvXXH0vHQ"
   },
   "outputs": [],
   "source": [
    "class AbstractAgent(metaclass=ABCMeta):\n",
    "    def init_actions(self, n_actions):\n",
    "        self._successes = np.zeros(n_actions)\n",
    "        self._failures = np.zeros(n_actions)\n",
    "        self._total_pulls = 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_action(self):\n",
    "        \"\"\"\n",
    "        Get current best action\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        \"\"\"\n",
    "        Observe reward from action and update agent's internal parameters\n",
    "        :type action: int\n",
    "        :type reward: int\n",
    "        \"\"\"\n",
    "        if reward == 1:\n",
    "          self._successes[action] += 1\n",
    "        else:\n",
    "          self._failures[action] += 1\n",
    "        self._total_pulls += 1\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "class RandomAgent(AbstractAgent):\n",
    "    def get_action(self):\n",
    "        np.random.randint(0, len(self._failures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytpRyo_A0vHR"
   },
   "source": [
    "### Epsilon-greedy agent\n",
    "\n",
    "**for** $t = 1,2,...$ **do**\n",
    "\n",
    "&nbsp;&nbsp; **for** $k = 1,...,K$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\hat\\theta_k \\leftarrow \\alpha_k / (\\alpha_k + \\beta_k)$\n",
    "\n",
    "&nbsp;&nbsp; **end for**\n",
    "\n",
    "&nbsp;&nbsp; $x_t \\leftarrow argmax_{k}\\hat\\theta$ with probability $1 - \\epsilon$ or random action with probability $\\epsilon$\n",
    "\n",
    "&nbsp;&nbsp; Apply $x_t$ and observe $r_t$\n",
    "\n",
    "&nbsp;&nbsp; $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow (\\alpha_{x_t}, \\beta_{x_t}) + (r_t, 1-r_t)$\n",
    "\n",
    "**end for**\n",
    "\n",
    "Implement the algorithm above in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "i6cDYFBO0vHR"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 5 (2191380527.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    @property\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 5\n"
     ]
    }
   ],
   "source": [
    "class EpsilonGreedyAgent(AbstractAgent):\n",
    "    def __init__(self, epsilon=0.01):\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "    def get_action(self):\n",
    "      # TODO Implement Epsilon Greedy\n",
    "\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__class__.__name__ + \"(epsilon={})\".format(self._epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzqWr5rT0vHR"
   },
   "source": [
    "### UCB Agent\n",
    "Epsilon-greedy strategy have no preference for actions. It would be better to select among actions that are uncertain or have potential to be optimal. One can come up with idea of index for each action that represents optimality and uncertainty at the same time. One efficient way to do it is to use UCB1 algorithm:\n",
    "\n",
    "**for** $t = 1,2,...$ **do**\n",
    "\n",
    "&nbsp;&nbsp; **for** $k = 1,...,K$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $w_k \\leftarrow \\alpha_k / (\\alpha_k + \\beta_k) + \\sqrt{2\\log(t) \\ / \\ (\\alpha_k + \\beta_k)}$\n",
    "\n",
    "&nbsp;&nbsp; **end for**\n",
    "\n",
    "&nbsp;&nbsp; **end for**\n",
    " $x_t \\leftarrow argmax_{k}w$\n",
    "\n",
    "&nbsp;&nbsp; Apply $x_t$ and observe $r_t$\n",
    "\n",
    "&nbsp;&nbsp; $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow (\\alpha_{x_t}, \\beta_{x_t}) + (r_t, 1-r_t)$\n",
    "\n",
    "**end for**\n",
    "\n",
    "__Note:__ in practice, one can multiply $\\sqrt{2\\log(t) \\ / \\ (\\alpha_k + \\beta_k)}$ by some tunable parameter to regulate agent's optimism and wilingness to abandon non-promising actions.\n",
    "\n",
    "More versions and optimality analysis - https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Jx8a3Rd0vHS"
   },
   "outputs": [],
   "source": [
    "class UCBAgent(AbstractAgent):\n",
    "    def get_action(self):\n",
    "        # TODO Implement UCB\n",
    "        return np.argmax(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIJTr4j10vHT"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def get_regret(env, agents, n_steps=5000, n_trials=50):\n",
    "    scores = OrderedDict({\n",
    "        agent.name: [0.0 for step in range(n_steps)] for agent in agents\n",
    "    })\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        env.reset()\n",
    "\n",
    "        for a in agents:\n",
    "            a.init_actions(env.action_count)\n",
    "\n",
    "        for i in range(n_steps):\n",
    "            optimal_reward = env.optimal_reward()\n",
    "\n",
    "            for agent in agents:\n",
    "                action = agent.get_action()\n",
    "                reward = env.pull(action)\n",
    "                agent.update(action, reward)\n",
    "                scores[agent.name][i] += optimal_reward - reward\n",
    "\n",
    "            env.step()  # change bandit's state if it is unstationary\n",
    "\n",
    "    for agent in agents:\n",
    "        scores[agent.name] = np.cumsum(scores[agent.name]) / n_trials\n",
    "\n",
    "    return scores\n",
    "\n",
    "def plot_regret(agents, scores):\n",
    "    for agent in agents:\n",
    "        plt.plot(scores[agent.name])\n",
    "\n",
    "    plt.legend([agent.name for agent in agents])\n",
    "\n",
    "    plt.ylabel(\"regret\")\n",
    "    plt.xlabel(\"steps\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2uISH8y0vHT"
   },
   "outputs": [],
   "source": [
    "# Uncomment agents\n",
    "agents = [\n",
    "    EpsilonGreedyAgent(),\n",
    "    UCBAgent(),\n",
    "]\n",
    "\n",
    "regret = get_regret(BernoulliBandit(), agents, n_steps=10000, n_trials=10)\n",
    "plot_regret(agents, regret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jt8emYUqq3Bt"
   },
   "source": [
    "### Bayesian Bandits\n",
    "* The difference between previously studied algorithms and Bayesian bandits algorithm is that Bayesian makes assumption about reward distrobution *R*.\n",
    "\n",
    "* Bayesian bandits alwasys exploits the proir knowledge ( ***p[R]*** ).\n",
    "* At timestep ***t*** it calculate the posterior probability distribution by taking into account the observed reward by taking action ***a*** ( ***p[R | ht]  = (a1, r2), ..., (at-1,rt-1)*** ).\n",
    "* **Note**: posterior knowledge at timestep ***t*** corresponds to proir knowledge at timestep ***t+1***   \n",
    "\n",
    "\n",
    "We continue with Bandit exmaple. We define the inital proir distribution as [*Beta*](https://towardsdatascience.com/beta-distribution-intuition-examples-and-derivation-cf00f4db57af) distribution. the **α** represents the number of successes and **β** represents the number of failure. We update **α** and **β** if the machine wins or loses respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkJthGrCChhf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BayesianBandit:\n",
    "    def __init__(self, num_machines):\n",
    "        self.num_machines = num_machines\n",
    "        self.alpha = np.ones(num_machines)    # prior of 1 for each machine\n",
    "        self.beta = np.ones(num_machines)     # prior of 1 for each machine\n",
    "        self.real_winning_rate = [round(np.random.random(), 1) for _ in range(num_machines)] # the real winning rate of machines. #### THIS PART IS UNKOWN FOR THE AGENT.\n",
    "\n",
    "    def choose_machine(self):\n",
    "        theta = np.random.beta(self.alpha, self.beta)    # sample theta values from Beta distribution\n",
    "        return np.argmax(theta)    # choose the machine with highest theta value\n",
    "\n",
    "    def update(self, machine, reward):\n",
    "        self.alpha[machine] += reward    # update alpha with reward\n",
    "        self.beta[machine] += 1 - reward    # update beta with non-reward\n",
    "\n",
    "    def take_action(self, machine):\n",
    "        return 1 if self.real_winning_rate[machine] > np.random.random()  else 0\n",
    "\n",
    "# Example usage:\n",
    "num_machines = 5\n",
    "num_trials = 1000\n",
    "bandit = BayesianBandit(num_machines)\n",
    "rewards = np.zeros(num_machines)\n",
    "\n",
    "for i in range(num_trials):\n",
    "    machine = bandit.choose_machine()\n",
    "    # reward = np.random.binomial(1, 0.5)    # simulate reward with 50% probability\n",
    "    reward = bandit.take_action(machine)\n",
    "    rewards[machine] += reward\n",
    "    bandit.update(machine, reward)\n",
    "\n",
    "# Plot the results:\n",
    "x = np.arange(num_machines)\n",
    "plt.bar(x, rewards)\n",
    "plt.title('Bayesian Bandit Results after {} trials'.format(num_trials))\n",
    "plt.xlabel('Option')\n",
    "plt.ylabel('Number of rewards')\n",
    "plt.show()\n",
    "# print([(a,b)for a,b in zip(bandit.alpha, bandit.beta)])\n",
    "print(bandit.real_winning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bn6-_aMyB9-V"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFB4hSJD-Lnl"
   },
   "outputs": [],
   "source": [
    "def plot_posteriors(priorR,priorG,priorB,ax=None,title=None):\n",
    "    #fig = plt.figure(figsize=(12.5, 10))\n",
    "    parameters = [priorR,priorG,priorB]\n",
    "    x = np.linspace(0.001, 1, 150)\n",
    "    for i, (alpha, beta) in enumerate(parameters):\n",
    "        color = assets[i]\n",
    "        y = stats.beta.pdf(x, alpha, beta)\n",
    "        lines = sns.lineplot(x, y, label=\"%s (%.1f,%.1f)\" % (color, alpha, beta), color = color,ax=ax)\n",
    "        plt.fill_between(x, 0, y, alpha=0.2, color=color)\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "        plt.autoscale(tight=True)\n",
    "    plt.legend(title=r\"$\\alpha, \\beta$ - parameters\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(40, 60))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "cnt=1\n",
    "for i in range(0,30,2):\n",
    "    ax = fig.add_subplot(8, 4, cnt)\n",
    "    g = plot_posteriors(*data[i][1:],ax,\"after \"+str(i)+\" runs\")\n",
    "    cnt+=1\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
