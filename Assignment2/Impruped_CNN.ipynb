{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29d14043-c2ea-48a9-af02-85433b65048e",
      "metadata": {
        "id": "29d14043-c2ea-48a9-af02-85433b65048e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from shutil import copyfile\n",
        "import copy\n",
        "import time\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import datasets, models\n",
        "import numpy as np\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4b74e6e-e7d5-4817-8f73-f1448c09a25b",
      "metadata": {
        "id": "b4b74e6e-e7d5-4817-8f73-f1448c09a25b"
      },
      "outputs": [],
      "source": [
        "# 1. Defining transformations for training data\n",
        "transform_1_train = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Normalize with ImageNet statistics\n",
        "])\n",
        "\n",
        "# 2. Defining transformations for test and validation data\n",
        "transform_1_test = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet statistics\n",
        "])\n",
        "\n",
        "# 3. Loading the training dataset with transformations applied\n",
        "train_model_1_dataset = datasets.Flowers102(root='data', split='train', download=True, transform = transform_1_train)\n",
        "val_model_1_dataset = datasets.Flowers102(root='data', split='val', download=True, transform = transform_1_test)\n",
        "test_model_1_dataset = datasets.Flowers102(root='data', split='test', download=True, transform = transform_1_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bde44a9d-c969-43b7-af98-8969c1de2e0c",
      "metadata": {
        "id": "bde44a9d-c969-43b7-af98-8969c1de2e0c"
      },
      "outputs": [],
      "source": [
        "# 1. Creating DataLoader for training , validation, test datasets\n",
        "train_model_1_dataloader = DataLoader(train_model_1_dataset, batch_size=32, shuffle=True)\n",
        "val_model_1_dataloader = DataLoader(val_model_1_dataset, batch_size=32, shuffle=False)\n",
        "test__model_1_dataloader = DataLoader(test_model_1_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3784a856-0dfc-44f8-95f0-6a4b0266d14e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3784a856-0dfc-44f8-95f0-6a4b0266d14e",
        "outputId": "a2cb0848-a49d-463a-e3d4-ad4b25ac8d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleCNN(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=131072, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=102, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)  # Layer 1\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Layer 2\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)  # Layer 3\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Layer 4\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)  # Layer 5\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # Layer 6\n",
        "\n",
        "        self.flatten = nn.Flatten()  # Flatten the tensor starting from the first dimension (batch dimension)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(in_features=128 * 32 * 32, out_features=512) # Layer 7\n",
        "        self.fc2 = nn.Linear(in_features=512, out_features=102)  # Layer 8 (output layer, 102 classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the network\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        x = self.fc2(x) # Output layer\n",
        "        return x\n",
        "\n",
        "# Create an instance of the model\n",
        "model1 = SimpleCNN()\n",
        "print(model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "209626df-310f-4fab-a0a4-60c7f0f688d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "209626df-310f-4fab-a0a4-60c7f0f688d2",
        "outputId": "b961ab37-335f-4d1c-a3bd-b7070d6a3ac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.67.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.7)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Installing tensorboard and torchinfo\n",
        "!pip install torchinfo\n",
        "!pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01668823-6133-47df-a727-21ad6aa3c450",
      "metadata": {
        "id": "01668823-6133-47df-a727-21ad6aa3c450"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "model1 = SimpleCNN()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model1.to(device) # Move the model to the specified device\n",
        "\n",
        "optimizer = optim.SGD(model1.parameters(), lr=0.001)  # Stochastic Gradient Descent (SGD) with learning rate of 0.001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize the TensorBoard writer to log metrics\n",
        "writer = SummaryWriter(log_dir='runs/flower_classification')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to train the model for one epoch\n",
        " Args:\n",
        "    - model (torch.nn.Module): The model to be trained.\n",
        "    - train_dataloader (DataLoader): DataLoader for the training set.\n",
        "    - criterion (torch.nn.Module): Loss function used for training.\n",
        "    - optimizer (torch.optim.Optimizer): Optimizer used for training.\n",
        "    - device (torch.device): The device (CPU or GPU) to train the model on.\n",
        "    - writer (SummaryWriter): TensorBoard writer for logging.\n",
        "    - epoch (int): The current epoch number.\n",
        "    \n",
        "    Returns:\n",
        "    - epoch_loss (float): The average loss for the epoch.\n",
        "    - epoch_accuracy (float): The accuracy for the epoch.\n",
        "    - epoch_f1_score (float): The F1 score for the epoch."
      ],
      "metadata": {
        "id": "0sdYNp_IDmFT"
      },
      "id": "0sdYNp_IDmFT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "511286b8-c61f-444a-bca7-4c64b22e01e3",
      "metadata": {
        "id": "511286b8-c61f-444a-bca7-4c64b22e01e3"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, train_dataloader, criterion, optimizer, device, writer, epoch):\n",
        "\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    all_train_preds = []\n",
        "    all_train_labels = []\n",
        "\n",
        "    # Loop through the training data\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero out the gradients\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Update prediction and label lists for F1 score calculation\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "        all_train_preds.extend(predicted.cpu().numpy())\n",
        "        all_train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate average loss, accuracy, and F1 score for the epoch\n",
        "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
        "    epoch_accuracy = correct_preds / total_preds\n",
        "    epoch_f1_score = f1_score(all_train_labels, all_train_preds, average='weighted')\n",
        "\n",
        "    # Log metrics to TensorBoard\n",
        "    writer.add_scalar('Training Loss', epoch_loss, epoch)\n",
        "    writer.add_scalar('Training Accuracy', epoch_accuracy, epoch)\n",
        "    writer.add_scalar('Training F1 Score', epoch_f1_score, epoch)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.4f}, Train F1 Score: {epoch_f1_score:.4f}')\n",
        "\n",
        "    return epoch_loss, epoch_accuracy, epoch_f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "meMa0mfXD2al"
      },
      "id": "meMa0mfXD2al"
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_one_epoch(model, val_dataloader, criterion, device, writer, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    val_loss = 0.0\n",
        "    correct_preds_val = 0\n",
        "    total_preds_val = 0\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_preds_val += (predicted == labels).sum().item()\n",
        "            total_preds_val += labels.size(0)\n",
        "\n",
        "            all_val_preds.extend(predicted.cpu().numpy())\n",
        "            all_val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "    val_loss = val_loss / len(val_dataloader.dataset)\n",
        "    val_accuracy = correct_preds_val / total_preds_val\n",
        "    val_f1_score = f1_score(all_val_labels, all_val_preds, average='weighted')\n",
        "\n",
        "\n",
        "    writer.add_scalar('Validation Loss', val_loss, epoch)\n",
        "    writer.add_scalar('Validation Accuracy', val_accuracy, epoch)\n",
        "    writer.add_scalar('Validation F1 Score', val_f1_score, epoch)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1_score:.4f}')\n",
        "\n",
        "    return val_loss, val_accuracy, val_f1_score"
      ],
      "metadata": {
        "id": "DOO6DGCm5WMF"
      },
      "id": "DOO6DGCm5WMF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, val_dataloader, test_dataloader, criterion, optimizer, device, num_epochs, writer):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "    train_f1_scores = []\n",
        "    val_f1_scores = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        epoch_loss, epoch_accuracy, epoch_f1_score = train_one_epoch(\n",
        "            model, train_dataloader, criterion, optimizer, device, writer, epoch)\n",
        "\n",
        "\n",
        "        val_loss, val_accuracy, val_f1_score = validate_one_epoch(\n",
        "            model, val_dataloader, criterion, device, writer, epoch)\n",
        "\n",
        "\n",
        "        train_losses.append(epoch_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accuracies.append(epoch_accuracy)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        train_f1_scores.append(epoch_f1_score)\n",
        "        val_f1_scores.append(val_f1_score)\n",
        "\n",
        "\n",
        "    test_loss, test_accuracy, test_f1_score = test_model(model, test_dataloader, criterion, device)\n",
        "\n",
        "    return (train_losses, val_losses, train_accuracies, val_accuracies,\n",
        "            train_f1_scores, val_f1_scores, test_loss, test_accuracy, test_f1_score)\n",
        "\n"
      ],
      "metadata": {
        "id": "s9K_qLFw5boi"
      },
      "id": "s9K_qLFw5boi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_dataloader, criterion, device):\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    correct_preds_test = 0\n",
        "    total_preds_test = 0\n",
        "    all_test_preds = []\n",
        "    all_test_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_preds_test += (predicted == labels).sum().item()\n",
        "            total_preds_test += labels.size(0)\n",
        "\n",
        "            all_test_preds.extend(predicted.cpu().numpy())\n",
        "            all_test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "    test_loss = test_loss / len(test_dataloader.dataset)\n",
        "    test_accuracy = correct_preds_test / total_preds_test\n",
        "    test_f1_score = f1_score(all_test_labels, all_test_preds, average='weighted')\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1_score:.4f}')\n",
        "\n",
        "    return test_loss, test_accuracy, test_f1_score"
      ],
      "metadata": {
        "id": "LODxsg2N5djW"
      },
      "id": "LODxsg2N5djW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses, train_accuracies, val_accuracies, train_f1_scores, val_f1_scores, test_loss, test_accuracy, test_f1_score = train_model(\n",
        "    model=model1,\n",
        "    train_dataloader = train_model_1_dataloader,\n",
        "    val_dataloader = val_model_1_dataloader,\n",
        "    test_dataloader = test__model_1_dataloader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    num_epochs=10,\n",
        "    writer=writer\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHdxaoY65fir",
        "outputId": "07ab30d1-d6b1-4fff-ac89-6ddf215a60df"
      },
      "id": "CHdxaoY65fir",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 4.6264, Train Accuracy: 0.0069, Train F1 Score: 0.0005\n",
            "Epoch 1, Validation Loss: 4.6232, Validation Accuracy: 0.0127, Validation F1 Score: 0.0014\n",
            "Epoch 2, Train Loss: 4.6223, Train Accuracy: 0.0098, Train F1 Score: 0.0015\n",
            "Epoch 2, Validation Loss: 4.6208, Validation Accuracy: 0.0196, Validation F1 Score: 0.0039\n",
            "Epoch 3, Train Loss: 4.6186, Train Accuracy: 0.0196, Train F1 Score: 0.0056\n",
            "Epoch 3, Validation Loss: 4.6185, Validation Accuracy: 0.0294, Validation F1 Score: 0.0068\n",
            "Epoch 4, Train Loss: 4.6149, Train Accuracy: 0.0333, Train F1 Score: 0.0076\n",
            "Epoch 4, Validation Loss: 4.6161, Validation Accuracy: 0.0294, Validation F1 Score: 0.0061\n",
            "Epoch 5, Train Loss: 4.6109, Train Accuracy: 0.0412, Train F1 Score: 0.0117\n",
            "Epoch 5, Validation Loss: 4.6135, Validation Accuracy: 0.0363, Validation F1 Score: 0.0082\n",
            "Epoch 6, Train Loss: 4.6071, Train Accuracy: 0.0490, Train F1 Score: 0.0179\n",
            "Epoch 6, Validation Loss: 4.6108, Validation Accuracy: 0.0402, Validation F1 Score: 0.0110\n",
            "Epoch 7, Train Loss: 4.6028, Train Accuracy: 0.0510, Train F1 Score: 0.0160\n",
            "Epoch 7, Validation Loss: 4.6079, Validation Accuracy: 0.0422, Validation F1 Score: 0.0168\n",
            "Epoch 8, Train Loss: 4.5982, Train Accuracy: 0.0569, Train F1 Score: 0.0219\n",
            "Epoch 8, Validation Loss: 4.6047, Validation Accuracy: 0.0490, Validation F1 Score: 0.0171\n",
            "Epoch 9, Train Loss: 4.5932, Train Accuracy: 0.0735, Train F1 Score: 0.0349\n",
            "Epoch 9, Validation Loss: 4.6012, Validation Accuracy: 0.0461, Validation F1 Score: 0.0156\n",
            "Epoch 10, Train Loss: 4.5880, Train Accuracy: 0.0814, Train F1 Score: 0.0438\n",
            "Epoch 10, Validation Loss: 4.5974, Validation Accuracy: 0.0451, Validation F1 Score: 0.0162\n",
            "Test Loss: 4.5995, Test Accuracy: 0.0298, Test F1 Score: 0.0143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bb69e43-ae2c-4dd0-b04b-49b8be5605a3",
      "metadata": {
        "id": "9bb69e43-ae2c-4dd0-b04b-49b8be5605a3"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir runs/flower_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "702a3060-51ae-4324-8d1e-10b8226f47f9",
      "metadata": {
        "id": "702a3060-51ae-4324-8d1e-10b8226f47f9"
      },
      "outputs": [],
      "source": [
        "# Очистка кеша GPU после тренировки модели\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3c268a8-4e14-454e-a8c3-f1ba69390b2e",
      "metadata": {
        "id": "a3c268a8-4e14-454e-a8c3-f1ba69390b2e"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([transforms.Resize((256,256)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                                ])\n",
        "# Трансформации для теста\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),               # Преобразование в тензор\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Нормализация\n",
        "])\n",
        "# Загрузка тренировочного и тестового датасетов\n",
        "train_model_2_dataset = datasets.Flowers102(root='data', split='train', download=True, transform= transform_train)\n",
        "val_model_2_dataset = datasets.Flowers102(root='data', split='val', download=True, transform= transform_test)\n",
        "test_model_2_dataset = datasets.Flowers102(root='data', split='test', download=True, transform= transform_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cefbc4d-d3ce-4efd-bf8f-eb5e2cb2ce51",
      "metadata": {
        "id": "6cefbc4d-d3ce-4efd-bf8f-eb5e2cb2ce51"
      },
      "outputs": [],
      "source": [
        "train_model_2_dataloader = DataLoader(train_model_2_dataset, batch_size=32, shuffle=True)\n",
        "val_model_2_dataloader = DataLoader(val_model_2_dataset, batch_size=32, shuffle=False)\n",
        "test_model_2_dataloader = DataLoader(test_model_2_dataset, batch_size=32,shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ed2c920-5655-4bb1-bbc7-bef3f4457ecc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ed2c920-5655-4bb1-bbc7-bef3f4457ecc",
        "outputId": "b81d8d2e-46dc-4d1a-f596-01cd6a99ffbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f0f9184-b3c2-4682-9db4-8ac8d767dbd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f0f9184-b3c2-4682-9db4-8ac8d767dbd9",
        "outputId": "b0b266da-d83b-4552-e838-ddc4288bf0a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ImprovedCNN(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=131072, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=102, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class ImprovedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedCNN, self).__init__()\n",
        "\n",
        "        # Слои свёрточной нейронной сети с Batch Normalization\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Полносвязный слой с Dropout\n",
        "        self.fc1 = nn.Linear(in_features=128 * 32 * 32, out_features=512)\n",
        "        self.fc2 = nn.Linear(in_features=512, out_features=102)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Прямой проход через сеть с Batch Normalization и Dropout\n",
        "        x = F.relu(self.bn1(self.conv1(x)))  # Применяем BN и ReLU\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.bn2(self.conv2(x)))  # Применяем BN и ReLU\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))  # Применяем BN и ReLU\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Создание модели\n",
        "model2 = ImprovedCNN()\n",
        "\n",
        "print(model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e87417-b8c9-4373-88e1-eae0bbbb3246",
      "metadata": {
        "id": "c1e87417-b8c9-4373-88e1-eae0bbbb3246"
      },
      "outputs": [],
      "source": [
        "# Создаём модель\n",
        "model2 = ImprovedCNN()\n",
        "\n",
        "# Убираем использование GPU (если доступен)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model2.to(device)\n",
        "\n",
        "# Оптимизатор\n",
        "optimizer = optim.SGD(model2.parameters(), lr=0.001)  # SGD momentum=0.9, weight_decay=1e-4\n",
        "\n",
        "# Функция потерь для многоклассовой классификации\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Логирование с использованием TensorBoard\n",
        "writer = SummaryWriter(log_dir='runs/flower_classification')  # Создаём директорию для TensorBoard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cd851ae-fbc4-4f3d-928c-ad8eb80aeadc",
      "metadata": {
        "id": "3cd851ae-fbc4-4f3d-928c-ad8eb80aeadc"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, train_dataloader, criterion, optimizer, device, writer, epoch):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    all_train_preds = []\n",
        "    all_train_labels = []\n",
        "\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Optimizer step\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)  # Accumulate loss\n",
        "        _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "        all_train_preds.extend(predicted.cpu().numpy())\n",
        "        all_train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Compute epoch metrics\n",
        "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
        "    epoch_accuracy = correct_preds / total_preds\n",
        "    epoch_f1_score = f1_score(all_train_labels, all_train_preds, average='weighted')\n",
        "\n",
        "    # Log metrics to TensorBoard\n",
        "    writer.add_scalar('Training Loss', epoch_loss, epoch)\n",
        "    writer.add_scalar('Training Accuracy', epoch_accuracy, epoch)\n",
        "    writer.add_scalar('Training F1 Score', epoch_f1_score, epoch)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, F1 Score: {epoch_f1_score:.4f}')\n",
        "\n",
        "    return epoch_loss, epoch_accuracy, epoch_f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aMybnx_AVtiQ",
      "metadata": {
        "id": "aMybnx_AVtiQ"
      },
      "outputs": [],
      "source": [
        "def validate_one_epoch(model, val_dataloader, criterion, device, writer, epoch):\n",
        "    model.eval()  # Switch to evaluation mode (turns off Dropout, BatchNorm)\n",
        "\n",
        "    val_loss = 0.0\n",
        "    correct_preds_val = 0\n",
        "    total_preds_val = 0\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation during validation\n",
        "        for inputs, labels in val_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)  # Accumulate loss\n",
        "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
        "            correct_preds_val += (predicted == labels).sum().item()\n",
        "            total_preds_val += labels.size(0)\n",
        "\n",
        "            all_val_preds.extend(predicted.cpu().numpy())\n",
        "            all_val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Compute epoch metrics\n",
        "    val_loss = val_loss / len(val_dataloader.dataset)\n",
        "    val_accuracy = correct_preds_val / total_preds_val\n",
        "    val_f1_score = f1_score(all_val_labels, all_val_preds, average='weighted')\n",
        "\n",
        "    # Log metrics to TensorBoard\n",
        "    writer.add_scalar('Validation Loss', val_loss, epoch)\n",
        "    writer.add_scalar('Validation Accuracy', val_accuracy, epoch)\n",
        "    writer.add_scalar('Validation F1 Score', val_f1_score, epoch)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, F1 Score: {val_f1_score:.4f}')\n",
        "\n",
        "    return val_loss, val_accuracy, val_f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4E47Cn_OWezL",
      "metadata": {
        "id": "4E47Cn_OWezL"
      },
      "outputs": [],
      "source": [
        "def test_model(model, test_dataloader, criterion, device):\n",
        "    model.eval()  # Переключаем модель в режим оценки\n",
        "\n",
        "    test_loss = 0.0\n",
        "    correct_preds_test = 0\n",
        "    total_preds_test = 0\n",
        "    all_test_preds = []\n",
        "    all_test_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Отключаем вычисление градиентов\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)  # Прямой проход\n",
        "            loss = criterion(outputs, labels)  # Вычисляем потерю\n",
        "\n",
        "            test_loss += loss.item() * inputs.size(0)  # Накопление потерь\n",
        "            _, predicted = torch.max(outputs, 1)  # Выбираем класс с максимальной вероятностью\n",
        "            correct_preds_test += (predicted == labels).sum().item()\n",
        "            total_preds_test += labels.size(0)\n",
        "\n",
        "            all_test_preds.extend(predicted.cpu().numpy())\n",
        "            all_test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Вычисляем метрики для теста\n",
        "    test_loss = test_loss / len(test_dataloader.dataset)\n",
        "    test_accuracy = correct_preds_test / total_preds_test\n",
        "    test_f1_score = f1_score(all_test_labels, all_test_preds, average='weighted')\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1_score:.4f}')\n",
        "\n",
        "    return test_loss, test_accuracy, test_f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TZVfKBRZVwcP",
      "metadata": {
        "id": "TZVfKBRZVwcP"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dataloader, val_dataloader, test_dataloader, criterion, optimizer, device, num_epochs, writer):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "    train_f1_scores = []\n",
        "    val_f1_scores = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Обучение модели на одной эпохе\n",
        "        train_loss, train_accuracy, train_f1_score = train_one_epoch(\n",
        "            model, train_dataloader, criterion, optimizer, device, writer, epoch\n",
        "        )\n",
        "\n",
        "        # Валидация модели на одной эпохе\n",
        "        val_loss, val_accuracy, val_f1_score = validate_one_epoch(\n",
        "            model, val_dataloader, criterion, device, writer, epoch\n",
        "        )\n",
        "\n",
        "        # Логирование метрик на каждой эпохе\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        train_f1_scores.append(train_f1_score)\n",
        "        val_f1_scores.append(val_f1_score)\n",
        "\n",
        "    # Тестирование модели после завершения всех эпох\n",
        "    test_loss, test_accuracy, test_f1_score = test_model(\n",
        "        model, test_dataloader, criterion, device\n",
        "    )\n",
        "\n",
        "    # Закрытие TensorBoard writer\n",
        "    writer.close()\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies, train_f1_scores, val_f1_scores, test_loss, test_accuracy, test_f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ORoR_FNPTsqf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORoR_FNPTsqf",
        "outputId": "384892c2-c7ab-4492-9ef1-c2736141a0f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 4.6528, Accuracy: 0.0196, F1 Score: 0.0138\n",
            "Epoch 1, Validation Loss: 4.5233, Accuracy: 0.0441, F1 Score: 0.0184\n",
            "Epoch 2, Train Loss: 4.3734, Accuracy: 0.0422, F1 Score: 0.0337\n",
            "Epoch 2, Validation Loss: 4.3145, Accuracy: 0.0657, F1 Score: 0.0342\n",
            "Epoch 3, Train Loss: 4.0980, Accuracy: 0.0922, F1 Score: 0.0774\n",
            "Epoch 3, Validation Loss: 4.1322, Accuracy: 0.1049, F1 Score: 0.0720\n",
            "Epoch 4, Train Loss: 3.7950, Accuracy: 0.1578, F1 Score: 0.1412\n",
            "Epoch 4, Validation Loss: 3.9589, Accuracy: 0.1422, F1 Score: 0.1025\n",
            "Epoch 5, Train Loss: 3.4312, Accuracy: 0.2451, F1 Score: 0.2230\n",
            "Epoch 5, Validation Loss: 3.8461, Accuracy: 0.1520, F1 Score: 0.1072\n",
            "Epoch 6, Train Loss: 3.1290, Accuracy: 0.3206, F1 Score: 0.2989\n",
            "Epoch 6, Validation Loss: 3.6778, Accuracy: 0.1902, F1 Score: 0.1434\n",
            "Epoch 7, Train Loss: 2.8044, Accuracy: 0.4039, F1 Score: 0.3873\n",
            "Epoch 7, Validation Loss: 3.6000, Accuracy: 0.2078, F1 Score: 0.1621\n",
            "Epoch 8, Train Loss: 2.4092, Accuracy: 0.5294, F1 Score: 0.5161\n",
            "Epoch 8, Validation Loss: 3.4543, Accuracy: 0.2275, F1 Score: 0.1913\n",
            "Epoch 9, Train Loss: 2.1101, Accuracy: 0.6098, F1 Score: 0.6053\n",
            "Epoch 9, Validation Loss: 3.3867, Accuracy: 0.2441, F1 Score: 0.2024\n",
            "Epoch 10, Train Loss: 1.8364, Accuracy: 0.6892, F1 Score: 0.6846\n",
            "Epoch 10, Validation Loss: 3.3209, Accuracy: 0.2490, F1 Score: 0.2110\n",
            "Test Loss: 3.4532, Test Accuracy: 0.2166, Test F1 Score: 0.1979\n"
          ]
        }
      ],
      "source": [
        "train_losses, val_losses, train_accuracies, val_accuracies, train_f1_scores, val_f1_scores, test_loss, test_accuracy, test_f1_score = train_model(\n",
        "    model = model2,\n",
        "    train_dataloader = train_model_2_dataloader,\n",
        "    val_dataloader = val_model_2_dataloader,\n",
        "    test_dataloader = test_model_2_dataloader,  # Передаем test_dataloader\n",
        "    criterion = criterion,\n",
        "    optimizer = optimizer,\n",
        "    device=device,\n",
        "    num_epochs=10,\n",
        "    writer=writer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OW7qee_xoD8s",
      "metadata": {
        "id": "OW7qee_xoD8s"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir runs/flower_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d4b84fc-b53e-4318-820d-c14c92acabae",
      "metadata": {
        "id": "4d4b84fc-b53e-4318-820d-c14c92acabae"
      },
      "outputs": [],
      "source": [
        "    # Очистка кеша GPU после тренировки модели\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "485e5e55-b7dd-48a3-b8f6-9af92e5ff037",
      "metadata": {
        "id": "485e5e55-b7dd-48a3-b8f6-9af92e5ff037"
      },
      "outputs": [],
      "source": [
        "# Определение преобразований\n",
        "transform_train = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "  # Crop the center 224x224 pixels\n",
        "    transforms.ToTensor(),  # Convert the image to a tensor\n",
        "    # Normalize with ImageNet mean and std\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),  # Crop the center 224x224 pixels\n",
        "    transforms.ToTensor(),              # Convert PIL image to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "])\n",
        "\n",
        "train_model_3_dataset = datasets.Flowers102(root='data', split='train', download=True, transform = transform_train)\n",
        "val_model_3_dataset = datasets.Flowers102(root='data', split='val', download=True, transform = transform_test)\n",
        "test_model_3_dataset = datasets.Flowers102(root='data', split='test', download=True, transform = transform_test)\n",
        "\n",
        "train_model_3_dataloader = DataLoader(train_model_3_dataset, batch_size=32, shuffle=True)\n",
        "val_model_3_dataloader = DataLoader(val_model_3_dataset, batch_size=32, shuffle=False)\n",
        "test_model_3_dataloader = DataLoader(test_model_3_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d942cd7-fde0-44e6-bb73-eb6f1b70ce50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d942cd7-fde0-44e6-bb73-eb6f1b70ce50",
        "outputId": "d90886af-4603-4cf5-efe6-926ce17d6af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=102, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "model_vgg16 = models.vgg16(pretrained=True)\n",
        "num_ftrs = model_vgg16.classifier[6].in_features\n",
        "model_vgg16.classifier[6] = nn.Linear(num_ftrs, 102)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_vgg16.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7m1yeUnTjNqU",
      "metadata": {
        "id": "7m1yeUnTjNqU"
      },
      "outputs": [],
      "source": [
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Замораживаем все параметры модели\n",
        "for param in model_vgg16.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Размораживаем только последние параметры для обучения\n",
        "for param in model_vgg16.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# model_vgg16.add(Dense(2,activation = 'softmax', name='output'))\n",
        "\n",
        "optimizer = optim.SGD(model_vgg16.classifier.parameters(), lr=0.001,momentum=0.9)\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# Setup TensorBoard logging\n",
        "writer = SummaryWriter()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_one_epoch(model, train_dataloader, criterion, optimizer, device, writer, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    all_train_preds = []\n",
        "    all_train_labels = []\n",
        "\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "                        # Reset gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Optimizer step\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "        all_train_preds.extend(predicted.cpu().numpy())\n",
        "        all_train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
        "    epoch_accuracy = correct_preds / total_preds\n",
        "    epoch_f1_score = f1_score(all_train_labels, all_train_preds, average='weighted')\n",
        "\n",
        "    # Log to TensorBoard\n",
        "    writer.add_scalar('Training Loss', epoch_loss, epoch)\n",
        "    writer.add_scalar('Training Accuracy', epoch_accuracy, epoch)\n",
        "    writer.add_scalar('Training F1 Score', epoch_f1_score, epoch)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.4f}, Train F1 Score: {epoch_f1_score:.4f}')\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    return epoch_loss, epoch_accuracy, epoch_f1_score"
      ],
      "metadata": {
        "id": "ZgNIL_Ga4Yvm"
      },
      "id": "ZgNIL_Ga4Yvm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_one_epoch(model, val_dataloader, criterion, device, writer, epoch):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_preds_val = 0\n",
        "    total_preds_val = 0\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_preds_val += (predicted == labels).sum().item()\n",
        "            total_preds_val += labels.size(0)\n",
        "\n",
        "            all_val_preds.extend(predicted.cpu().numpy())\n",
        "            all_val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss = val_loss / len(val_dataloader.dataset)\n",
        "    val_accuracy = correct_preds_val / total_preds_val\n",
        "    val_f1_score = f1_score(all_val_labels, all_val_preds, average='weighted')\n",
        "\n",
        "    writer.add_scalar('Validation Loss', val_loss, epoch)\n",
        "    writer.add_scalar('Validation Accuracy', val_accuracy, epoch)\n",
        "    writer.add_scalar('Validation F1 Score', val_f1_score, epoch)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1_score:.4f}')\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    return val_loss, val_accuracy, val_f1_score"
      ],
      "metadata": {
        "id": "dbhOog77E502"
      },
      "id": "dbhOog77E502",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # Training step\n",
        "    train_loss, train_accuracy, train_f1_score = train_one_epoch(\n",
        "        model=model_vgg16,\n",
        "        train_dataloader=train_model_3_dataloader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        writer=writer,\n",
        "        epoch=epoch\n",
        "    )\n",
        "\n",
        "    # Validation step\n",
        "    val_loss, val_accuracy, val_f1_score = validate_one_epoch(\n",
        "        model=model_vgg16,\n",
        "        val_dataloader=val_model_3_dataloader,\n",
        "        criterion=criterion,\n",
        "        device=device,\n",
        "        writer=writer,\n",
        "        epoch=epoch\n",
        "    )\n",
        "\n",
        "# 11. **Close the TensorBoard writer**\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3jnUSB8E7nf",
        "outputId": "b2b1da8f-502e-450c-de30-cb3beda784bb"
      },
      "id": "T3jnUSB8E7nf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 4.5458, Train Accuracy: 0.0500, Train F1 Score: 0.0429\n",
            "Epoch 1, Validation Loss: 4.0053, Validation Accuracy: 0.2598, Validation F1 Score: 0.2245\n",
            "Epoch 2, Train Loss: 3.5692, Train Accuracy: 0.2794, Train F1 Score: 0.2492\n",
            "Epoch 2, Validation Loss: 3.0184, Validation Accuracy: 0.5490, Validation F1 Score: 0.5062\n",
            "Epoch 3, Train Loss: 2.4203, Train Accuracy: 0.5441, Train F1 Score: 0.5202\n",
            "Epoch 3, Validation Loss: 2.0652, Validation Accuracy: 0.6147, Validation F1 Score: 0.5931\n",
            "Epoch 4, Train Loss: 1.7241, Train Accuracy: 0.6696, Train F1 Score: 0.6569\n",
            "Epoch 4, Validation Loss: 1.9387, Validation Accuracy: 0.6775, Validation F1 Score: 0.6592\n",
            "Epoch 5, Train Loss: 1.5886, Train Accuracy: 0.7314, Train F1 Score: 0.7233\n",
            "Epoch 5, Validation Loss: 1.8410, Validation Accuracy: 0.7098, Validation F1 Score: 0.6957\n",
            "Epoch 6, Train Loss: 1.4722, Train Accuracy: 0.7716, Train F1 Score: 0.7655\n",
            "Epoch 6, Validation Loss: 1.8331, Validation Accuracy: 0.7147, Validation F1 Score: 0.7009\n",
            "Epoch 7, Train Loss: 1.4889, Train Accuracy: 0.7598, Train F1 Score: 0.7505\n",
            "Epoch 7, Validation Loss: 1.8251, Validation Accuracy: 0.7147, Validation F1 Score: 0.7000\n",
            "Epoch 8, Train Loss: 1.4577, Train Accuracy: 0.7598, Train F1 Score: 0.7522\n",
            "Epoch 8, Validation Loss: 1.8173, Validation Accuracy: 0.7157, Validation F1 Score: 0.7008\n",
            "Epoch 9, Train Loss: 1.4613, Train Accuracy: 0.7657, Train F1 Score: 0.7599\n",
            "Epoch 9, Validation Loss: 1.8165, Validation Accuracy: 0.7147, Validation F1 Score: 0.6999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77d02f3c-1cef-4ad4-a57d-fe15140a16b7",
      "metadata": {
        "id": "77d02f3c-1cef-4ad4-a57d-fe15140a16b7"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir runs/flower_classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DIEZHfpSRkRJ"
      },
      "id": "DIEZHfpSRkRJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}