{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7510c84-6bb5-4fb9-b5d5-693760f17aee",
   "metadata": {},
   "source": [
    "# Lab Assignment: Linear Regression and High-Dimensional Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7169d2dd-d63b-4671-b598-df3370969e05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7948445-b4b8-480b-913a-e935ee3191fc",
   "metadata": {},
   "source": [
    "1. **Loading and Visualizing the Dataset**:\n",
    "   - Use the **California Housing** dataset to predict house prices based on various features such as **median income** and **number of households**.\n",
    "\n",
    "   **Exercise**:\n",
    "   - Load the dataset, visualize the first few rows, and plot the relationship between selected features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef96e3-f858-4ed8-9404-39d5f4452559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "cali = fetch_california_housing(as_frame=True)\n",
    "df = cali.frame\n",
    "\n",
    "# Visualize data\n",
    "df.plot(kind='scatter', x='MedInc', y='MedHouseVal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a400e31-ee4a-4abe-8f27-e4b3a079786d",
   "metadata": {},
   "source": [
    "2. **Implementing Linear Regression with Least Squares (Normal Equation)**:\n",
    "   - Implement **Linear Regression** using the **Normal Equation**: $( \\theta = (X^T X)^{-1} X^T y )$.\n",
    "   - Use the dataset to solve for $( \\theta )$ using matrix operations.\n",
    "\n",
    "   **Exercise**:\n",
    "   - Implement the normal equation method and calculate the parameters $( \\theta )$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecab6909-6366-4b80-99f0-5127fc405bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def least_squares(X, y):\n",
    "    # Todo\n",
    "    return ...\n",
    "\n",
    "# Prepare the data (add intercept term)\n",
    "X = np.c_[np.ones(df.shape[0]), df['MedInc'].values]\n",
    "y = df['MedHouseVal'].values\n",
    "\n",
    "# Calculate theta\n",
    "theta = least_squares(X, y)\n",
    "print(\"Theta:\", theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d65e79-eb69-4513-bb16-193e9ee88cbd",
   "metadata": {},
   "source": [
    "## Part 2: Linear Regression using SVD\n",
    "\n",
    "1. **Using Singular Value Decomposition (SVD) for Linear Regression**:\n",
    "   - Instead of using the normal equation, solve linear regression using **SVD**. This method is numerically more stable, especially for ill-conditioned or high-dimensional matrices.\n",
    "   \n",
    "   **Exercise**:\n",
    "   - Implement linear regression using the **SVD** decomposition of the matrix \\( X \\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f8867a-2ecb-4b44-8891-6c3885047b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_least_squares(X, y):\n",
    "    # Todo\n",
    "    U, S, Vt = ...\n",
    "    theta_svd = ...\n",
    "    return theta_svd\n",
    "\n",
    "# Calculate theta using SVD\n",
    "theta_svd = svd_least_squares(X, y)\n",
    "print(\"Theta (SVD):\", theta_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d04793f-a384-4c3c-8033-afe831dd955d",
   "metadata": {},
   "source": [
    "## Part 3: Shortcut (Scikit-learn)\n",
    "\n",
    "1. **Introducing Scikit-learn's Linear Regression**:\n",
    "   - Compare your manually implemented Linear Regression (Normal Equation and SVD) with the **Scikit-learn's Linear Regression** model. Use the built-in model for performance comparison.\n",
    "\n",
    "   **Exercise**:\n",
    "   - Use `sklearn.linear_model.LinearRegression` to solve the regression problem and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26885113-5789-46d4-a95f-6f8db80569de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Todo\n",
    "model = ...\n",
    "model.fit(X, y)\n",
    "print(\"Scikit-learn coefficients:\", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5483a6e6-5991-42e4-806d-174a9bcd11e8",
   "metadata": {},
   "source": [
    "## Part 4: Gradient Descent for Large Datasets\n",
    "\n",
    "1. **Issues with Direct Methods on Large Datasets**:\n",
    "   - Demonstrate the performance issue with the **Normal Equation** on very large datasets by creating synthetic data.\n",
    "\n",
    "   **Exercise**:\n",
    "   - Generate a large dataset and time the normal equation solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf27701-f672-4fae-8bb9-e858fcf460c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn import datasets\n",
    "\n",
    "# Generate large synthetic dataset\n",
    "X_large, y_large, w_true = datasets.make_regression(\n",
    "    n_samples=100000, # number of points\n",
    "    n_features=1500, # number of features\n",
    "    n_informative=1000, # number of informative features\n",
    "    noise=1.0, # std of y\n",
    "    bias=0, # bias for the model\n",
    "    coef=True, # returning true coeffs\n",
    "    random_state=2024\n",
    ")\n",
    "\n",
    "# Time the normal equation solution\n",
    "start_time = time.time()\n",
    "theta_large = least_squares(X_large, y_large)\n",
    "print(\"Time taken by normal equation:\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965ec42-088a-40b4-947d-8672f0c01e5a",
   "metadata": {},
   "source": [
    "2. **Implementing Gradient Descent**:\n",
    "   - Introduce **Batch Gradient Descent** as an alternative to handle large datasets efficiently.\n",
    "   \n",
    "   **Exercise**:\n",
    "   - Implement gradient descent to solve the same problem iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7713f-7428-4ba6-87ee-e3d9e929e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X, y, alpha=0.01, iterations=1000, batch_size=32):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    for i in range(iterations):\n",
    "        idx = np.random.choice(m, batch_size, replace=False)\n",
    "        X_batch = X[idx]\n",
    "        y_batch = y[idx]\n",
    "        # Todo\n",
    "        gradient = ...\n",
    "        theta = ...\n",
    "    return theta\n",
    "\n",
    "# Run Mini-Batch Gradient Descent\n",
    "start_time = time.time()\n",
    "theta_mbgd = mini_batch_gradient_descent(X_large, y_large, alpha=0.1, iterations=500, batch_size=32)\n",
    "print(\"Time taken by Gradient Descent:\", time.time() - start_time)\n",
    "print(\"Difference from true values :\", np.mean(abs(theta_mbgd-w_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32d039-ef97-452e-ae6f-7dffb143ddf8",
   "metadata": {},
   "source": [
    "## Part 5: High-Dimensional Linear Regression\n",
    "\n",
    "1. **Real-World Dataset with Many Features**:\n",
    "   - Use a **real-world high-dimensional dataset** (e.g., **Gene Expression Data** or **1000 Genomes Dataset** from the UCI repository or Kaggle). A suitable high-dimensional dataset is the **\"Arcene\"** dataset (from UCI) with 10,000 features.\n",
    "\n",
    "   **Exercise**:\n",
    "   - Download [Arcane](https://archive.ics.uci.edu/dataset/167/arcene) dataset and apply linear regression to solve for $( \\theta )$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa04a57-b3ed-4b7e-a8d0-7104e35148eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "arcene_data = pd.read_csv('arcene/ARCENE/arcene_train.data', sep=' ', header=None)\n",
    "arcene_data = arcene_data.drop(columns=[10000])  # Remove last empty column\n",
    "\n",
    "# Load training labels\n",
    "arcene_labels = pd.read_csv('arcene/ARCENE/arcene_train.labels', sep=' ', header=None)\n",
    "\n",
    "print(\"Shape of training data:\", arcene_data.shape)\n",
    "print(\"Shape of training labels:\", arcene_labels.shape)\n",
    "\n",
    "# Implement least squares on high-dimensional dataset\n",
    "X_hd = arcene_data.values\n",
    "y_hd = arcene_labels.values.ravel()\n",
    "\n",
    "# Solve using normal equation\n",
    "theta_hd = least_squares(X_hd, y_hd)\n",
    "print(\"Theta for high-dimensional data:\", theta_hd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565eac73-2328-4088-9dfb-d50e8490abff",
   "metadata": {},
   "source": [
    "2. **Ridge Regression for High-Dimensional Data**:\n",
    "   - Introduce **Ridge Regression (L2 Regularization)** as a solution to overfitting and multicollinearity in high-dimensional data.\n",
    "\n",
    "   **Exercise**:\n",
    "   - Implement Ridge Regression using the formula $( \\theta = (X^T X + \\lambda I)^{-1} X^T y )$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc273a1-9cb3-4c48-9a81-3e27e24ff419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X, y, lambda_):\n",
    "   m, n = X.shape\n",
    "   I = np.eye(n)\n",
    "   return np.linalg.inv(X.T @ X + lambda_ * I) @ X.T @ y\n",
    "\n",
    "# Apply Ridge Regression\n",
    "theta_ridge = ridge_regression(X_hd, y_hd, lambda_=0.1)\n",
    "print(\"Theta (Ridge):\", theta_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e132b89-5eed-4215-930d-b5c9b2aff5d8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. **Linear Regression using different methods**: You implemented Linear Regression using the **Normal Equation**, **SVD**, and **Gradient Descent**.\n",
    "2. **Comparison**: You compared the manual implementations with **scikit-learnâ€™s** linear regression.\n",
    "3. **High-Dimensional Data**: You applied linear regression techniques to a real-world **high-dimensional dataset** (Arcene) and explored how **Ridge Regression** can handle overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
