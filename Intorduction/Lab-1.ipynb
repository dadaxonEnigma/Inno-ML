{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Week 1 : Data Manipulation and Exploration\n",
    "```\n",
    "- Machine Learning, Innopolis University (Fall semester 2024)\n",
    "- Instructors: Adil Khan & Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "\n",
    "### Content\n",
    "```\n",
    "Lab Plan\n",
    "1. Data exploration\n",
    "2. Dealing with categorical features\n",
    "3. Dealing with missing data\n",
    "4. Features Scaling\n",
    "5. Trainset splitting\n",
    "6. Data Visualization\n",
    "\n",
    "```\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "![](https://www.gosmar.eu/machinelearning/wp-content/uploads/2021/01/MLOps_pipeline_scaling3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for libraries installation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read Data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "football_df = pd.read_csv('./football_data.csv', low_memory=False)\n",
    "\n",
    "football_df.drop(['Date', 'time'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GameID</th>\n",
       "      <th>Drive</th>\n",
       "      <th>qtr</th>\n",
       "      <th>down</th>\n",
       "      <th>TimeUnder</th>\n",
       "      <th>TimeSecs</th>\n",
       "      <th>PlayTimeDiff</th>\n",
       "      <th>SideofField</th>\n",
       "      <th>yrdln</th>\n",
       "      <th>yrdline100</th>\n",
       "      <th>...</th>\n",
       "      <th>yacEPA</th>\n",
       "      <th>Home_WP_pre</th>\n",
       "      <th>Away_WP_pre</th>\n",
       "      <th>Home_WP_post</th>\n",
       "      <th>Away_WP_post</th>\n",
       "      <th>Win_Prob</th>\n",
       "      <th>WPA</th>\n",
       "      <th>airWPA</th>\n",
       "      <th>yacWPA</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016121101</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>SD</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016121101</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SD</td>\n",
       "      <td>47.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330475</td>\n",
       "      <td>0.968025</td>\n",
       "      <td>0.031975</td>\n",
       "      <td>0.940170</td>\n",
       "      <td>0.059830</td>\n",
       "      <td>0.031975</td>\n",
       "      <td>0.027855</td>\n",
       "      <td>0.024299</td>\n",
       "      <td>0.003556</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016121101</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>CAR</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.940170</td>\n",
       "      <td>0.059830</td>\n",
       "      <td>0.947575</td>\n",
       "      <td>0.052425</td>\n",
       "      <td>0.059830</td>\n",
       "      <td>-0.007405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016121101</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1877.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>CAR</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.766159</td>\n",
       "      <td>0.947575</td>\n",
       "      <td>0.052425</td>\n",
       "      <td>0.953601</td>\n",
       "      <td>0.046399</td>\n",
       "      <td>0.052425</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>0.036136</td>\n",
       "      <td>-0.042161</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016121101</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>CAR</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.953601</td>\n",
       "      <td>0.046399</td>\n",
       "      <td>0.912354</td>\n",
       "      <td>0.087646</td>\n",
       "      <td>0.046399</td>\n",
       "      <td>0.041247</td>\n",
       "      <td>0.041247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       GameID  Drive  qtr  down  TimeUnder  TimeSecs  PlayTimeDiff  \\\n",
       "0  2016121101     13    2   NaN          2    1920.0           5.0   \n",
       "1  2016121101     13    2   1.0          2    1920.0           0.0   \n",
       "2  2016121101     13    2   1.0          2    1910.0          10.0   \n",
       "3  2016121101     13    2   2.0          2    1877.0          33.0   \n",
       "4  2016121101     13    2   3.0          2    1868.0           9.0   \n",
       "\n",
       "  SideofField  yrdln  yrdline100  ...    yacEPA  Home_WP_pre  Away_WP_pre  \\\n",
       "0          SD   44.0        44.0  ...       NaN          NaN          NaN   \n",
       "1          SD   47.0        53.0  ...  0.330475     0.968025     0.031975   \n",
       "2         CAR    7.0         7.0  ...       NaN     0.940170     0.059830   \n",
       "3         CAR    9.0         9.0  ... -3.766159     0.947575     0.052425   \n",
       "4         CAR    9.0         9.0  ...  0.000000     0.953601     0.046399   \n",
       "\n",
       "   Home_WP_post Away_WP_post  Win_Prob       WPA    airWPA    yacWPA  Season  \n",
       "0           NaN          NaN  0.000000       NaN       NaN       NaN    2016  \n",
       "1      0.940170     0.059830  0.031975  0.027855  0.024299  0.003556    2016  \n",
       "2      0.947575     0.052425  0.059830 -0.007405       NaN       NaN    2016  \n",
       "3      0.953601     0.046399  0.052425 -0.006025  0.036136 -0.042161    2016  \n",
       "4      0.912354     0.087646  0.046399  0.041247  0.041247  0.000000    2016  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "football_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number categorical featues: 36\n",
      "GameID         int64\n",
      "Drive          int64\n",
      "qtr            int64\n",
      "down         float64\n",
      "TimeUnder      int64\n",
      "              ...   \n",
      "Win_Prob     float64\n",
      "WPA          float64\n",
      "airWPA       float64\n",
      "yacWPA       float64\n",
      "Season         int64\n",
      "Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "types = football_df.dtypes\n",
    "print(\"Number categorical featues:\", sum(types=='object'))\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GameID</th>\n",
       "      <th>Drive</th>\n",
       "      <th>qtr</th>\n",
       "      <th>down</th>\n",
       "      <th>TimeUnder</th>\n",
       "      <th>TimeSecs</th>\n",
       "      <th>PlayTimeDiff</th>\n",
       "      <th>yrdln</th>\n",
       "      <th>yrdline100</th>\n",
       "      <th>ydstogo</th>\n",
       "      <th>...</th>\n",
       "      <th>yacEPA</th>\n",
       "      <th>Home_WP_pre</th>\n",
       "      <th>Away_WP_pre</th>\n",
       "      <th>Home_WP_post</th>\n",
       "      <th>Away_WP_post</th>\n",
       "      <th>Win_Prob</th>\n",
       "      <th>WPA</th>\n",
       "      <th>airWPA</th>\n",
       "      <th>yacWPA</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>8448.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>9992.000000</td>\n",
       "      <td>9984.000000</td>\n",
       "      <td>9980.000000</td>\n",
       "      <td>9980.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3929.000000</td>\n",
       "      <td>9357.000000</td>\n",
       "      <td>9357.000000</td>\n",
       "      <td>9309.000000</td>\n",
       "      <td>9309.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>9.845000e+03</td>\n",
       "      <td>3930.000000</td>\n",
       "      <td>3923.000000</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.016372e+09</td>\n",
       "      <td>12.444900</td>\n",
       "      <td>2.584500</td>\n",
       "      <td>1.991951</td>\n",
       "      <td>7.322400</td>\n",
       "      <td>1686.264011</td>\n",
       "      <td>20.739283</td>\n",
       "      <td>28.566433</td>\n",
       "      <td>47.859218</td>\n",
       "      <td>7.158400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.400785</td>\n",
       "      <td>0.547408</td>\n",
       "      <td>0.453126</td>\n",
       "      <td>0.547667</td>\n",
       "      <td>0.452708</td>\n",
       "      <td>0.470946</td>\n",
       "      <td>1.673133e-03</td>\n",
       "      <td>0.014596</td>\n",
       "      <td>-0.009764</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.996451e+05</td>\n",
       "      <td>7.075561</td>\n",
       "      <td>1.131542</td>\n",
       "      <td>1.006399</td>\n",
       "      <td>4.685216</td>\n",
       "      <td>1064.437980</td>\n",
       "      <td>20.795996</td>\n",
       "      <td>12.533362</td>\n",
       "      <td>24.737534</td>\n",
       "      <td>4.844632</td>\n",
       "      <td>...</td>\n",
       "      <td>2.008014</td>\n",
       "      <td>0.300660</td>\n",
       "      <td>0.300744</td>\n",
       "      <td>0.302591</td>\n",
       "      <td>0.302650</td>\n",
       "      <td>0.319106</td>\n",
       "      <td>4.684680e-02</td>\n",
       "      <td>0.056720</td>\n",
       "      <td>0.065689</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.016121e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-849.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.156367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.719480e-01</td>\n",
       "      <td>-0.943119</td>\n",
       "      <td>-0.939447</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.016122e+09</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.950799</td>\n",
       "      <td>0.298087</td>\n",
       "      <td>0.183309</td>\n",
       "      <td>0.295898</td>\n",
       "      <td>0.179571</td>\n",
       "      <td>0.172491</td>\n",
       "      <td>-1.375776e-02</td>\n",
       "      <td>-0.009758</td>\n",
       "      <td>-0.017638</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.016122e+09</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.565156</td>\n",
       "      <td>0.435096</td>\n",
       "      <td>0.568334</td>\n",
       "      <td>0.431754</td>\n",
       "      <td>0.486653</td>\n",
       "      <td>-7.985673e-07</td>\n",
       "      <td>0.004151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.017010e+09</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2575.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469628</td>\n",
       "      <td>0.817714</td>\n",
       "      <td>0.702465</td>\n",
       "      <td>0.820549</td>\n",
       "      <td>0.705156</td>\n",
       "      <td>0.750169</td>\n",
       "      <td>1.278280e-02</td>\n",
       "      <td>0.033391</td>\n",
       "      <td>0.009991</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.017010e+09</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>908.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.079540</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.739204e-01</td>\n",
       "      <td>0.898214</td>\n",
       "      <td>0.932194</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             GameID         Drive           qtr         down     TimeUnder  \\\n",
       "count  1.000000e+04  10000.000000  10000.000000  8448.000000  10000.000000   \n",
       "mean   2.016372e+09     12.444900      2.584500     1.991951      7.322400   \n",
       "std    3.996451e+05      7.075561      1.131542     1.006399      4.685216   \n",
       "min    2.016121e+09      1.000000      1.000000     1.000000      0.000000   \n",
       "25%    2.016122e+09      6.000000      2.000000     1.000000      3.000000   \n",
       "50%    2.016122e+09     12.000000      3.000000     2.000000      7.000000   \n",
       "75%    2.017010e+09     18.000000      4.000000     3.000000     11.000000   \n",
       "max    2.017010e+09     30.000000      5.000000     4.000000     15.000000   \n",
       "\n",
       "          TimeSecs  PlayTimeDiff        yrdln   yrdline100       ydstogo  ...  \\\n",
       "count  9992.000000   9984.000000  9980.000000  9980.000000  10000.000000  ...   \n",
       "mean   1686.264011     20.739283    28.566433    47.859218      7.158400  ...   \n",
       "std    1064.437980     20.795996    12.533362    24.737534      4.844632  ...   \n",
       "min    -849.000000      0.000000     1.000000     1.000000      0.000000  ...   \n",
       "25%     772.000000      5.000000    19.000000    29.000000      3.000000  ...   \n",
       "50%    1800.000000     18.000000    30.000000    48.000000      9.000000  ...   \n",
       "75%    2575.000000     37.000000    38.000000    69.000000     10.000000  ...   \n",
       "max    3600.000000    908.000000    50.000000    99.000000     36.000000  ...   \n",
       "\n",
       "            yacEPA  Home_WP_pre  Away_WP_pre  Home_WP_post  Away_WP_post  \\\n",
       "count  3929.000000  9357.000000  9357.000000   9309.000000   9309.000000   \n",
       "mean     -0.400785     0.547408     0.453126      0.547667      0.452708   \n",
       "std       2.008014     0.300660     0.300744      0.302591      0.302650   \n",
       "min     -12.156367     0.000000     0.000000      0.000000      0.000000   \n",
       "25%      -0.950799     0.298087     0.183309      0.295898      0.179571   \n",
       "50%       0.000000     0.565156     0.435096      0.568334      0.431754   \n",
       "75%       0.469628     0.817714     0.702465      0.820549      0.705156   \n",
       "max       8.079540     1.000000     1.000000      1.000000      1.000000   \n",
       "\n",
       "           Win_Prob           WPA       airWPA       yacWPA   Season  \n",
       "count  10000.000000  9.845000e+03  3930.000000  3923.000000  10000.0  \n",
       "mean       0.470946  1.673133e-03     0.014596    -0.009764   2016.0  \n",
       "std        0.319106  4.684680e-02     0.056720     0.065689      0.0  \n",
       "min        0.000000 -9.719480e-01    -0.943119    -0.939447   2016.0  \n",
       "25%        0.172491 -1.375776e-02    -0.009758    -0.017638   2016.0  \n",
       "50%        0.486653 -7.985673e-07     0.004151     0.000000   2016.0  \n",
       "75%        0.750169  1.278280e-02     0.033391     0.009991   2016.0  \n",
       "max        1.000000  9.739204e-01     0.898214     0.932194   2016.0  \n",
       "\n",
       "[8 rows x 64 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "football_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Profiling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "# pip install ydata-profiling\n",
    "\n",
    "report = ProfileReport(football_df)\n",
    "# report.to_file('data_profile_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding of categorical feature\n",
    "1. Why to encode the categorical feature?\n",
    "2. Why one-hot-encoding form? why not give each category in the feature a specific code value?\n",
    "3. How many new features will be added? What if we have some missing values, which is the case in this dataset?\n",
    "4. When is it appropriate to give each category an encoding value?\n",
    "\n",
    "\n",
    "<span style=\"color:red\"> Task : In the next cell, implement the function that take a dataframe, name of the categorical feature, and the encoder object. Then, adds new features that represent the one-hot-encoding form of this feature and ignore the missing values in it (encode them to zeros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# cat_feat = football_df.select_dtypes(include=['object']).columns.tolist()\n",
    "# encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "# encoder.fit(football_df[cat_feat])\n",
    "# encoded_data = encoder.transform(football_df[cat_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (553984053.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[111], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    ,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode_feature(df, feature_name, encoder):\n",
    "    df[feature_name].fillna('missing',inplace = True)\n",
    "    encoded_features = encoder.transform(df[feature_name])\n",
    "    encoded_df = pd.DataFrame(encoded_features.toarray(), \n",
    "                               , \n",
    "                               index=df.index)\n",
    "    df_encoded = pd.concat([df, encoded_df], axis=1)\n",
    "    df_encoded.drop(columns=[feature_name], inplace=True)\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "input_features is not equal to feature_names_in_",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder(handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m encoder\u001b[38;5;241m.\u001b[39mfit(football_df[cat_feats])\n\u001b[1;32m----> 4\u001b[0m football_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mone_hot_encode_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfootball_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcat_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[107], line 5\u001b[0m, in \u001b[0;36mone_hot_encode_feature\u001b[1;34m(df, feature_name, encoder)\u001b[0m\n\u001b[0;32m      2\u001b[0m df[feature_name]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m,inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m encoded_features \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(df[feature_name])\n\u001b[0;32m      4\u001b[0m encoded_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(encoded_features\u001b[38;5;241m.\u001b[39mtoarray(), \n\u001b[1;32m----> 5\u001b[0m                            columns\u001b[38;5;241m=\u001b[39m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[0;32m      6\u001b[0m                            index\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m      7\u001b[0m df_encoded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, encoded_df], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m df_encoded\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[feature_name], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mE:\\anaconda\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1207\u001b[0m, in \u001b[0;36mOneHotEncoder.get_feature_names_out\u001b[1;34m(self, input_features)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get output feature names for transformation.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \n\u001b[0;32m   1189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;124;03m    Transformed feature names.\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1206\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1207\u001b[0m input_features \u001b[38;5;241m=\u001b[39m \u001b[43m_check_feature_names_in\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1208\u001b[0m cats \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_transformed_categories(i)\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_)\n\u001b[0;32m   1211\u001b[0m ]\n\u001b[0;32m   1213\u001b[0m name_combiner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_get_feature_name_combiner()\n",
      "File \u001b[1;32mE:\\anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py:2280\u001b[0m, in \u001b[0;36m_check_feature_names_in\u001b[1;34m(estimator, input_features, generate_names)\u001b[0m\n\u001b[0;32m   2276\u001b[0m input_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(input_features, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m   2277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_names_in_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(\n\u001b[0;32m   2278\u001b[0m     feature_names_in_, input_features\n\u001b[0;32m   2279\u001b[0m ):\n\u001b[1;32m-> 2280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features is not equal to feature_names_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features_in_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_features) \u001b[38;5;241m!=\u001b[39m n_features_in_:\n\u001b[0;32m   2283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features should have length equal to number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2285\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2286\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: input_features is not equal to feature_names_in_"
     ]
    }
   ],
   "source": [
    "cat_feats = football_df.select_dtypes(include=['object']).columns.tolist()\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder.fit(football_df[cat_feats])\n",
    "football_encoded = one_hot_encode_feature(football_df,cat_feats,encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Imputation\n",
    "#### Imputation is filling the missing values in the dataset.\n",
    "Several issues to address:\n",
    "1. Why we should fill them? Why not remove the rows that has missing values?\n",
    "2. Filling it with constant value for each value, but what's it?\n",
    "3. (Advanced) Estimate the filling value, each row for its own.\n",
    "4. How to choose the imputation strategy?\n",
    "5. (Not related only to imputations) Why we fit the imputer to the training data not the concatenation of both train and test set?\n",
    "\n",
    "\n",
    "<span style=\"color:red\">Task :  In the next cell, implement a function that returns the count of nan/empty cells in a dataframe.</span><br>\n",
    "<span style=\"color:red\">Task : Use SimpleImputer object, fit it to the trainset, then transform both the train and test sets.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "#TODO: Complete a function that returns number of empty cells \n",
    "def count_nans(df):\n",
    "    # 1 or 2 lines.\n",
    "    return None\n",
    "\n",
    "# Print number of empty cells (1 line)\n",
    "print(\"#Empty missing data cells in the dataset = \", count_nans(encoded_data))\n",
    "\n",
    "# Write your code here (3-4 lines):\n",
    "# create the imputer object.\n",
    "# fit the imputer.\n",
    "# transform the data\n",
    "\n",
    "print(\"After Imputing:\")\n",
    "# Print number of empty cells in the data(1 line)\n",
    "print(\"#Empty cells in dataset =\", count_nans(imputed_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Features Scaling\n",
    "1. Why we need to make all the features have same range of values?\n",
    "<span style=\"color:blue\"> This has something to do with some machine learning algorithms.\n",
    "First, gradient-related algorithms (e.g., linear regression, logisitic regression, deep learning algorithms): feature scaling is important for the congergence of the algorithms as the gradient in one direction/feature could need more/long steps than the others and the scaling solves this issue and make the training stable and relatively faster.\n",
    "Second, distance related algorithms (e.g., KNN, K-means). These algorithms depends on the distance between samples both in training and testing. Having features with different ranges would affect the distance measure and produce biased prediciton to the features with larger range.\n",
    "for example: having two features (length in KM [10: 10000] and temperature in CÂ° [20: 40]). The distance value will be much affected by the length feature.</span>\n",
    "\n",
    "1. What are the different strategies to scale the features? <br>\n",
    "    * <span style=\"color:blue\">Min-Max scaling: transform all features to fall in this range: [0, 1].</span><br>\n",
    "$$x_i = \\frac{x_i - min(\\mathbf{x})}{max(\\mathbf{x}) - min(\\mathbf{x})}$$\n",
    "    * <span style=\"color:blue\">Standard scaling: transform all features to have mean = 0 and standard deviation = 1. And by assuming that all features follow normal distribution, we can say that standard scaling turns featrues to be $\\sim N(0,1)$.</span> <br>\n",
    "$$x_i = \\frac{x_i - mean(\\mathbf{x})}{stdev(\\mathbf{x})}$$ <br>\n",
    "    * <span style=\"color:blue\">Robust scaling: robust to the outliers that may affect the previous methods in calculating their mean, stdev, min, or max. It uses the interquartile range to scale the features according to it.\n",
    "Interquartile is the range between the 1st quartile and 3rd quartile.</span> <br>\n",
    "$$x_i = \\frac{x_i - Q_1(\\mathbf{x})}{Q_3(\\mathbf{x}) - Q_1(\\mathbf{x})}$$ \n",
    "\n",
    "3. How to choose the strategy?\n",
    "<span style=\"color:blue\">Same as in imputation strategy, using a validation set or using cross-validation.\n",
    "\n",
    "<span style=\"color:red\">  In the following cell, scale all the features with a scaler from your choice. Fit the scaler on the full data set and transform both it.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Write your code here (4 lines):\n",
    "# choose the scaler class.\n",
    "# create the scaler object.\n",
    "# fit it to the data.\n",
    "# transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6. Trainset splitting\n",
    "Issues that lead to trainset splitting:\n",
    "1. How we will measure the performance of our model?\n",
    "<span style=\"color:blue\"> Measuring performance on the trainset is not a good idea intuitively, as to estimate the generalization error, we need to evaluate the model on a data that wasn't seen before.\n",
    "So we need to split the dataset that we have to two parts trainset and testset.\n",
    "2. How we will tune the hyperparamters? Can it be done with testset?\n",
    "<span style=\"color:blue\"> If we tuned the hyperparamters on the testset, this will be considered as Data leakage as the hyperparams were tuned on the set that represent the evaluation, so there's a high chance that it will overfit the testset and fails to generalize (i.e., get high test accuracy but fails in production for example).\n",
    "So, it is a must to split the data into 3 parts:\n",
    "(70%) trainset to train the params , (10%) validation set to tune the hyperparameters (it can be used for other stuff like early stopping (tbe)), and (20%) testset to evaluate the model finally.\n",
    "\n",
    "Issues to think of:\n",
    "1. Is is the best strategy to get the best estimate for the true risk/performance of the system?\n",
    "<span style=\"color:blue\"> There's a chance that the testset is biased. So, a better way to estimate the true generalization error is to use cross validation.\n",
    "Cross validation is a method where the dataset, is divided into k equally sized folds then we train on k-1 folds and test on the left one and repeat this process k times for each fold and get k estimates, finally we average all of these k accuracies to represent the final accuracy.\n",
    "2. (Advanced) How to choose between two models?\n",
    "If you have two models one with test accuracy 90% and the other with 91% (averaged from cross valdiation), which one to choose?\n",
    "<span style=\"color:blue\"> The naive/fast approach is to use the model with higher accuracy. But what if these estimates are not significantly different (we got them by chance), then a hypothesis test needs to be done.\n",
    "After testing each model for each fold from the k-folds we can use \"Paired t-test\" to test if they are significantly different or not.\n",
    "<br />(more advanced way, which is used in industry: A/B testing, watch about it [here](https://www.youtube.com/watch?v=zFMgpxG-chM))\n",
    "\n",
    "<span style=\"color:red\"> TASK : In the next task, you will split your data into 3 parts, train, test, and validation by ratios: 70%, 20%, and 10% respectively.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: Write your code here (2 lines)\n",
    "\n",
    "print(x_train.shape, x_test.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 7. Data Visualization\n",
    "Visualization is key-issue to get some insights and intuition about your dataset.\n",
    "How to plot 259 features (259-dimensional data)? what we can plot -at maximum- is 2d or 3d data.\n",
    "\n",
    "Hint: We should reduce the dimension. Read this [article](https://towardsdatascience.com/dimensionality-reduction-ways-and-intuitions-1b5e97592d8e).\n",
    "\n",
    "So, Let's use UMAP to reduce the dimension of this dataset to be 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "dim_reducer = PCA(n_components=2)\n",
    "x_train_reduced = dim_reducer.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the reduced dataset on a 2D plane.\n",
    "Use matplotlib to make a scatter plot for the reduced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Write code to plot the reduced dataset in a scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 8. Self Practice task\n",
    "\n",
    "Using the Football dataset follow the steps below:\n",
    "1. Select `Win_Prob` as target variable \n",
    "1. Remove columns with constant value (i.e all the values in the column are equal to zero)\n",
    "1. Extract features from `Date` and `time` column\n",
    "1. Remove all columns with more than 99% missing values\n",
    "1. Remove all columns with 99% distinct values\n",
    "1. Split the data into train (80%) and test(20%) sets. \n",
    "1. Split the train data into train (80%) and validation sets. \n",
    "1. Encode categorical data using a different encoder (not One Hot encoder) : see [Category Encoders](https://contrib.scikit-learn.org/category_encoders/index.html) for a full list. remember that you fit the encoder on train data only and then transform test data\n",
    "1. Impute missing values\n",
    "\n",
    "<span style=\"color:red\"> NOTE : Make use of the insights from pandas profiling report</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
